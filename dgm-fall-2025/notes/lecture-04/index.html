<!DOCTYPE html>
<html>
  <head>
    <head>
  <meta charset="UTF-8">
  <meta http-equiv="content-language" content="en">
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>STAT 453 | Lecture 04 - Conditional Independence and Directed GMs (BNs)</title>
  <meta name="description" content="STAT 453 - University of Wisconsin-Madison - Fall 2025
">

  <link rel="shortcut icon" href="/dgm-fall-2025/assets/img/favicon.ico">

  <link rel="stylesheet" href="/dgm-fall-2025/assets/css/main.css">
  <link rel="canonical" href="/dgm-fall-2025/notes/lecture-04/">

  
  <!-- Load Latex JS -->
  <script src="https://cdn.jsdelivr.net/npm/latex.js@0.11.1/dist/latex.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/latex.js@0.11.1/dist/latex.component.js"></script>
  
</head>

    <script src="/dgm-fall-2025/assets/js/distillpub/template.v2.js"></script>
    <script src="/dgm-fall-2025/assets/js/distillpub/transforms.v2.js"></script>
  </head>

  <d-front-matter>
    <script type="text/json">{
      "title": "Lecture 04 - Conditional Independence and Directed GMs (BNs)",
      "description": "Review of conditional independence, and an introduction to Directed GMs (BNs)",
      "published": "January 30, 2025",
      "lecturers": [
        
        {
          "lecturer": "Ben Lengerich",
          "lecturerURL": "https://lengerichlab.github.io/"
        }
        
      ],
      "authors": [
        
        {
          "author": "Amy Cai"
        },
        
        {
          "author": "Nancy Liu"
        }
        
      ],
      "editors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body>

    <header class="site-header">

  <div class="wrapper">

    <span class="site-title">
      <a class="page-link" href="https://adaptinfer.org/dgm-fall-2025/">STAT 453</a>
    </span>

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <a class="page-link" href="/dgm-fall-2025/logistics/">logistics</a>
        <a class="page-link" href="/dgm-fall-2025/lectures/">lectures</a>
        <a class="page-link" href="/dgm-fall-2025/notes/">notes</a>
        <a class="page-link" href="/dgm-fall-2025/calendar/">calendar</a>
        <a class="page-link" href="/dgm-fall-2025/homework/">homework</a>
        <a class="page-link" href="/dgm-fall-2025/project/">project</a>
      </div>
    </nav>

  </div>

</header>



    <div class="page-content">

      <d-title>
        <h1>Lecture 04 - Conditional Independence and Directed GMs (BNs)</h1>
        <p>Review of conditional independence, and an introduction to Directed GMs (BNs)</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <ul>
  <li><a href="#logistics-review"><strong>Logistics Review</strong></a></li>
  <li><a href="#homework-info"><strong>Homework info</strong></a></li>
  <li><a href="#conditional-independence"><strong>Conditional Independence</strong></a></li>
  <li><a href="#naïve-bayes-classifier"><strong>Naïve Bayes Classifier</strong></a></li>
  <li><a href="#bayesian-networks-bns"><strong>Bayesian Networks (BNs)</strong></a></li>
  <li><a href="#hidden-markov-models-hmms"><strong>Hidden Markov Models (HMMs)</strong></a></li>
</ul>

<h2 id="logistics-review">Logistics Review</h2>

<ul>
  <li><strong>Class webpage</strong>: <a href="https://lengerichlab.github.io/pgm-spring-2025">lengerichlab.github.io/pgm-spring-2025</a></li>
  <li><a href="https://docs.google.com/spreadsheets/d/1-Mj0MwkSxidVe-HfnMZyUIk4N8cwMeuGzEYTrgDjKqk/edit?gid=0">Lecture scribe sign-up sheet</a></li>
  <li><strong>Readings,Class Announcements, Assignment Submissions</strong>: <a href="https://canvas.wisc.edu/courses/447453">Canvas</a></li>
  <li><strong>Instructor</strong>: Ben Lengerich
    <ul>
      <li>Office Hours: Thursday 3:30-4:30pm, 7278 Medical Sciences Center</li>
      <li>Email: <a href="mailto:lengerich@wisc.edu">lengerich@wisc.edu</a></li>
    </ul>
  </li>
  <li><strong>TA</strong>: Chenyang Jiang
    <ul>
      <li>Office Hours: Monday 11am-12pm, 1219 Medical Sciences Center</li>
      <li>Email: <a href="mailto:cjiang77@wisc.edu">cjiang77@wisc.edu</a></li>
    </ul>
  </li>
</ul>

<h2 id="homework-info">HomeWork info</h2>
<ul>
  <li><strong>Released</strong>: Due February 11 at midnight.
    <ul>
      <li>PDF and Latex solution template (<code class="language-plaintext highlighter-rouge">.tex</code>) available on website.</li>
    </ul>
  </li>
  <li><strong>Submit via</strong>: <a href="https://canvas.wisc.edu/courses/447453/assignments">Canvas</a></li>
  <li><strong>Most preferred format</strong>:
    <ul>
      <li>PDF with your solution written in the provided solution box using LaTeX.</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="conditional-independence"><strong>Conditional Independence</strong></h2>

<h3 id="definitions"><strong>Definitions</strong></h3>
<ul>
  <li><strong>Independence</strong>:<br />
Variables $X$ and $Y$ are independent $(X \perp Y)$ if:
    <ul>
      <li>$P(X, Y) = P(X)P(Y)$</li>
    </ul>
  </li>
  <li><strong>Conditional Independence</strong>:<br />
$X$ and $Y$ are conditionally independent given $Z$ if:
    <ul>
      <li>$P(X, Y \mid Z) = P(X \mid Z)P(Y \mid Z)$</li>
    </ul>
  </li>
</ul>

<figure id="Figure for conditional independence" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/dgm-fall-2025/assets/img/notes/lecture-04/ConditionalIndenpendence.png" style="width:50%; max-width:800px;" />
    </div>
  </div>
  </figure>

<h3 id="example"><strong>Example</strong></h3>
<ul>
  <li><strong>Medical Diagnosis</strong>:<br />
Let $X = \text{Fever}, Y = \text{Rash}, Z = \text{Measles}$.<br />
If a patient has measles ($Z$), knowing they have a fever ($X$) provides no additional information about whether they develop a rash ($Y$).</li>
</ul>

<figure id="Example of conditional independence figure" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/dgm-fall-2025/assets/img/notes/lecture-04/exampleCI.png" style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

<h3 id="relate-to-naïve-bayes"><strong>Relate to Naïve Bayes</strong></h3>
<ul>
  <li>Conditional independence allows us to compute $P(X\mid Y)$ efficiently.</li>
  <li>Switching the direction of one arrow does not change the probability.</li>
  <li>Switching the direction of <strong>two</strong> arrow changes the probability because there is a double-count evidence (two $X$’s repeatedly contains part of information about $Y$)</li>
</ul>

<figure id="Figure for Naïve Bayes (1)" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/dgm-fall-2025/assets/img/notes/lecture-04/Na%C3%AFveBayes(1).png" style="width:50%; max-width:800px;" />
    </div>
  </div>
  </figure>

<figure id="Figure for Naïve Bayes (2)" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/dgm-fall-2025/assets/img/notes/lecture-04/Na%C3%AFveBayes(2).png" style="width:50%; max-width:800px;" />
    </div>
  </div>
  </figure>

<figure id="Figure for Naïve Bayes (3)" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/dgm-fall-2025/assets/img/notes/lecture-04/Na%C3%AFveBayes(3).png" style="width:50%; max-width:800px;" />
    </div>
  </div>
  </figure>

<figure id="Figure for double-count evidence" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/dgm-fall-2025/assets/img/notes/lecture-04/double-count.png" style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

<hr />

<h2 id="directed-graphical-models-causality-relationship"><strong>Directed Graphical Models (causality relationship)</strong></h2>
<p>Two types of GMs:</p>
<ul>
  <li>Directed edges give causality relationship (e.g. Bayesian Network)</li>
  <li>Undirected edges give correlations between variables (e.g. Markov Random Field)
    <h3 id="1-markov-chains"><strong>1. Markov Chains</strong></h3>
  </li>
  <li><strong>Markov Property</strong>:<br />
The future state depends only on the current state:
    <ul>
      <li>$P(Z_{t+1} \mid Z_t, Z_{t-1}, \ldots) = P(Z_{t+1} \mid Z_t)$</li>
    </ul>
  </li>
  <li><strong>Transition Matrix</strong>:<br />
Defines probabilities $P(Z_t \mid Z_{t-1})$.</li>
  <li><strong>Application</strong>:<br />
Modeling sequences like weather patterns or stock prices.</li>
</ul>

<figure id="Figure for Markov Property" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/dgm-fall-2025/assets/img/notes/lecture-04/Directed%20PGM.png" style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

<h3 id="2-hidden-markov-models-hmms"><strong>2. Hidden Markov Models (HMMs)</strong></h3>
<p>We need it when the underlying drivers are not observed</p>
<ul>
  <li><strong>Components</strong>:
    <ul>
      <li><strong>Hidden States</strong> ($Z_t$): Latent variables (e.g., emotional states in speech).</li>
      <li><strong>Observations</strong> ($X_t$): Observed data (e.g., audio signals).</li>
      <li><strong>Transition Probability</strong>: $P(Z_t \mid Z_{t-1})$.</li>
      <li><strong>Emission Probability</strong>: $P(X_t \mid Z_t)$.</li>
    </ul>
  </li>
  <li><strong>Example</strong>:<br />
<strong>Dishonest Casino</strong>:
    <ul>
      <li>Hidden states: Fair die ($Z=0$) vs. loaded die ($Z=1$).</li>
      <li>Observations: Dice rolls (e.g., $X=6$).</li>
      <li>Goal: Infer when the dealer switches dice based on observed rolls.</li>
    </ul>
  </li>
</ul>
<figure id="Figure for HMM" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/dgm-fall-2025/assets/img/notes/lecture-04/Hidden%20Markov%20Model%20(HMM).png" style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

<h3 id="3-bayesian-networks-bns"><strong>3. Bayesian Networks (BNs)</strong></h3>
<ul>
  <li><strong>Structure</strong>:<br />
A BN is a <strong>directed acyclic graph</strong> whose nodes represent the random variables and whose edges represent direct influence of one variable on another. Provides the skeleton for representing a joint distribution compactly in a <strong>factorized</strong> way. It compacts representation of a set of <strong>conditional independence</strong> assumptions. We can view the graph as encoding a <strong>generative sampling process</strong> executed by nature.</li>
  <li><strong>Factorization</strong>:<br />
Joint distribution factorizes as:
    <ul>
      <li>$P(X_1, \ldots, X_n) = \prod_{i=1}^n P(X_i \mid \text{Parents}(X_i))$</li>
    </ul>
  </li>
  <li><strong>Key Structures</strong>:
    <ul>
      <li><strong>Common Parent</strong>:<br />
$A \leftarrow B \rightarrow C$ ⟹ $A \perp C \mid B$.<br />
<em>Example</em>: $B = \text{Season}, A = \text{Rain}, C = \text{Sprinkler}$.</li>
    </ul>
  </li>
</ul>
<figure id="Figure for common parent" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/dgm-fall-2025/assets/img/notes/lecture-04/common%20parent.png" style="width:50%; max-width:800px;" />
    </div>
  </div>
  </figure>

<ul>
  <li><strong>Cascade</strong>:<br />
$A \rightarrow B \rightarrow C$ ⟹ $A \perp C \mid B$.<br />
<em>Example</em>: $A = \text{Smoking}, B = \text{Lung Damage}, C = \text{Cough}$.</li>
</ul>
<figure id="Figure for cascade" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/dgm-fall-2025/assets/img/notes/lecture-04/Cascade.png" style="width:50%; max-width:800px;" />
    </div>
  </div>
  </figure>

<ul>
  <li><strong>V-Structure (Collider)</strong>:<br />
$A \rightarrow B \leftarrow C$ ⟹ $A$ and $C$ become dependent if $B$ is observed.<br />
<em>Example</em>: $A = \text{Alarm}, B = \text{Burglary}, C = \text{Earthquake}$.</li>
</ul>
<figure id="Figure for V-structure" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/dgm-fall-2025/assets/img/notes/lecture-04/V%20-%20structure.png" style="width:50%; max-width:800px;" />
    </div>
  </div>
  </figure>

<ul>
  <li><strong>I-map</strong>
    <ul>
      <li><strong>Independence set</strong>: let $P$ be a distribution on $X$. Define $I(P)$ to be the set of independences $(X \perp Y \mid Z)$ that hold in $P$.</li>
      <li><strong>I-Map</strong>: Let $G$ be any graph object with an associated independence set $I(G)$. We say that $G$ is an <strong>I-map</strong> for an independent set $I$ if $I(G) \subseteq I$.</li>
      <li><strong>I-Map Distribution</strong>: We say $G$ is an I-map for $P$ if $G$ is an I-map for $I(P)$, when we use $I(G)$ as the associated independence set.</li>
    </ul>
  </li>
</ul>
<figure id="Figure for I-map" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/dgm-fall-2025/assets/img/notes/lecture-04/I%20-%20Maps.png" style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

<ul>
  <li>Facts:
    <ul>
      <li>Any independence that $G$ asserts must hold in $P$. Conversely, $P$ could contain other independence that are not in $G$.</li>
      <li>We are capable to use $G$ to estimate $P$.</li>
      <li>Due to the property of BNs, $I(G)$ can be used to represent the local Markov assumption by $I_l(G)={X_i \perp NonDescendants_{X_i} \mid Pa_{X_i}:\forall i}$</li>
    </ul>
  </li>
</ul>
<figure id="Figure for NonDescendants" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/dgm-fall-2025/assets/img/notes/lecture-04/NonDescendants.png" style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

<ul>
  <li>Example: $G_0$, $G_1$, $G_2$ can be I-map for $P_1$, while $G_0$ is not the I-map for $P_2$. Because $G$ implies independence that is not contained in $P_2$, so it cannot be used to estimate $P_2$.</li>
</ul>

<figure id="Figure for I-map example" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/dgm-fall-2025/assets/img/notes/lecture-04/I%20Map%20example.png" style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

<ul>
  <li><strong>D-Separation</strong>:
    <ul>
      <li>A path between $X$ and $Y$ is <strong>blocked</strong> by $Z$ if:
        <ol>
          <li><strong>Chain</strong> $\rightarrow \bullet \rightarrow$: Middle node is in $Z$.</li>
          <li><strong>Fork</strong> $\leftarrow \bullet \rightarrow$: Middle node is in $Z$.</li>
          <li><strong>Collider</strong> $\rightarrow \bullet \leftarrow$: Middle node <em>and its descendants</em> are not in $Z$.</li>
        </ol>
      </li>
    </ul>
  </li>
</ul>
<figure id="Figure for chain" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/dgm-fall-2025/assets/img/notes/lecture-04/chain.png" style="width:50%; max-width:800px;" />
    </div>
  </div>
  </figure>

<figure id="Figure for Common Cause" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/dgm-fall-2025/assets/img/notes/lecture-04/Common%20Cause.png" style="width:50%; max-width:800px;" />
    </div>
  </div>
  </figure>

<figure id="Figure for collider" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/dgm-fall-2025/assets/img/notes/lecture-04/collider.png" style="width:50%; max-width:800px;" />
    </div>
  </div>
  </figure>

<ul>
  <li><strong>Definition</strong>: $X$ and $Y$ are d-separated given $Z$ if all paths are blocked.</li>
  <li><strong>MAG definition of D-Separation</strong>: Variable $X$ and $Y$ are D-separated given $Z$ if they are separated in the moralized ancestral graph.</li>
</ul>
<figure id="Figure for D-separation" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/dgm-fall-2025/assets/img/notes/lecture-04/D-separation.png" style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

<ul>
  <li><strong>Example</strong>:</li>
</ul>
<figure id="Figure for I(G) graph" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/dgm-fall-2025/assets/img/notes/lecture-04/I(G)%20graph.png" style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

<hr />

<h2 id="learning--inference"><strong>Learning &amp; Inference</strong></h2>

<h3 id="learning-in-bns"><strong>Learning in BNs</strong></h3>
<ul>
  <li><strong>Parameter Learning</strong>:<br />
Estimate Conditional Probability Tables (CPTs) from data:
    <ul>
      <li>$P(X_i \mid \text{Parents}(X_i)) = \frac{\text{Count}(X_i, \text{Parents}(X_i))}{\text{Count}(\text{Parents}(X_i))}$</li>
    </ul>
  </li>
  <li><strong>Structure Learning</strong>:<br />
Use algorithms like <strong>K2</strong> or <strong>PC</strong> to infer the DAG from data.</li>
</ul>

<h3 id="inference-in-bns"><strong>Inference in BNs</strong></h3>
<ul>
  <li><strong>Exact Inference</strong>:
    <ul>
      <li><strong>Variable Elimination</strong>: Marginalize variables step-by-step.</li>
      <li><strong>Junction Tree Algorithm</strong>: Transform the BN into a tree structure.</li>
    </ul>
  </li>
  <li><strong>Approximate Inference</strong>:
    <ul>
      <li><strong>Sampling</strong>: Markov Chain Monte Carlo (MCMC).</li>
      <li><strong>Loopy Belief Propagation</strong>: Message-passing in cyclic graphs.</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="i-equivalence"><strong>I-Equivalence</strong></h2>

<h3 id="definition"><strong>Definition</strong></h3>
<ul>
  <li>Two Bayesian Networks are <strong>I-equivalent</strong> if they encode the same set of conditional independence statements.</li>
  <li><strong>Example</strong>:<br />
Networks $A \rightarrow B \rightarrow C$ and $A \leftarrow B \leftarrow C$ are I-equivalent.</li>
</ul>
<figure id="Figure for I-equivalence" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/dgm-fall-2025/assets/img/notes/lecture-04/UniquesnessBNs.png" style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

<h3 id="implications"><strong>Implications</strong></h3>
<ul>
  <li>Different graph structures can represent identical independence relationships.</li>
  <li>Critical for model selection and avoiding overfitting.</li>
</ul>

<h3 id="notation-plate"><strong>Notation: “Plate”</strong></h3>
<ul>
  <li>Naïve Bayes with Streamlined Notation:</li>
</ul>
<figure id="Figure for Plate" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/dgm-fall-2025/assets/img/notes/lecture-04/Plate.png" style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>
<hr />

<h2 id="applications"><strong>Applications</strong></h2>

<h3 id="1-naïve-bayes-classifier"><strong>1. Naïve Bayes Classifier</strong></h3>
<ul>
  <li><strong>Assumption</strong>: Features are conditionally independent given the class.</li>
  <li><strong>Formula</strong>:
    <ul>
      <li>$P(Y \mid X_1, \ldots, X_n) \propto P(Y) \prod_{i=1}^n P(X_i \mid Y)$</li>
    </ul>
  </li>
  <li><strong>Use Case</strong>: Spam detection, sentiment analysis.</li>
</ul>

<h3 id="2-hidden-markov-models"><strong>2. Hidden Markov Models</strong></h3>
<ul>
  <li><strong>Applications</strong>:
    <ul>
      <li>Speech recognition (mapping audio to words).</li>
      <li>Bioinformatics (gene prediction from DNA sequences).</li>
    </ul>
  </li>
</ul>

<h3 id="3-causal-inference"><strong>3. Causal Inference</strong></h3>
<ul>
  <li><strong>Bayesian Networks for Causality</strong>:
    <ul>
      <li>Identify causal effects using interventions.</li>
      <li>Example: Estimating the effect of a drug on recovery while controlling for confounders.</li>
    </ul>
  </li>
</ul>

<hr />

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Introduction to Deep Learning and Generative Models</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Ben Lengerich</li><li><a class="u-email" href="mailto:TBD@wisc.edu">TBD@wisc.edu</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/AdaptInfer" target="_blank"><i class="fab fa-github"></i> <span class="username">AdaptInfer</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>&copy; Copyright 2025 University of Wisconsin. <br />
        Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

  <d-bibliography src="/dgm-fall-2025/assets/bibliography/2025-01-30-lecture-04.bib">
  </d-bibliography>

  <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/dgm-fall-2025/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.1/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.1/katex.min.js"></script>
<script src="/dgm-fall-2025/assets/js/katex.js"></script>



<!-- Load Anchor JS -->
<script src="//cdnjs.cloudflare.com/ajax/libs/anchor-js/3.2.2/anchor.min.js"></script>
<script>
  anchors.options.visible = 'hover';
  anchors.add('article h2, article h3, article h4, article h5, article h6');
</script>



<!-- Adjust LaTeX JS -->
<script src="/dgm-fall-2025/assets/js/latex.js"></script>


<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/dgm-fall-2025/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/dgm-fall-2025/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', '', 'auto');
ga('send', 'pageview');
</script>


</html>
