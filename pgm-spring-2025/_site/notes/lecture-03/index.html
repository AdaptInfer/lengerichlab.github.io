<!DOCTYPE html>
<html>
  <head>
    <head>
  <meta charset="UTF-8">
  <meta http-equiv="content-language" content="en">
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>STAT 479 PGMs | Lecture 03 - A Linear View of Discriminative and Generative Models</title>
  <meta name="description" content="10-708 - Probabilistic Graphical Models - University of Wisconsin-Madison - Spring 2025
">

  <link rel="shortcut icon" href="/pgm-spring-2025/assets/img/favicon.ico">

  <link rel="stylesheet" href="/pgm-spring-2025/assets/css/main.css">
  <link rel="canonical" href="/pgm-spring-2025/notes/lecture-03/">

  
  <!-- Load Latex JS -->
  <script src="https://cdn.jsdelivr.net/npm/latex.js@0.11.1/dist/latex.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/latex.js@0.11.1/dist/latex.component.js"></script>
  
</head>

    <script src="/pgm-spring-2025/assets/js/distillpub/template.v2.js"></script>
    <script src="/pgm-spring-2025/assets/js/distillpub/transforms.v2.js"></script>
  </head>

  <d-front-matter>
    <script type="text/json">{
      "title": "Lecture 03 - A Linear View of Discriminative and Generative Models",
      "description": "Introduction to LaTex & Distinction between Discriminative and Generative Models",
      "published": "January 28, 2025",
      "lecturers": [
        
        {
          "lecturer": "Ben Lengerich"
        },
        
        {
          "lecturer": "",
          "lecturerURL": "https://lengerichlab.github.io/"
        }
        
      ],
      "authors": [
        
        {
          "author": "Tianjia Li"
        },
        
        {
          "author": "Brian Slupecki"
        }
        
      ],
      "editors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body>

    <header class="site-header">

  <div class="wrapper">

    <span class="site-title">
      <a class="page-link" href="https://adaptinfer.org/pgm-spring-2025/">STAT 479 PGMs</a>
    </span>

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <a class="page-link" href="/pgm-spring-2025/logistics/">logistics</a>
        <a class="page-link" href="/pgm-spring-2025/lectures/">lectures</a>
        <a class="page-link" href="/pgm-spring-2025/notes/">notes</a>
        <a class="page-link" href="/pgm-spring-2025/calendar/">calendar</a>
        <a class="page-link" href="/pgm-spring-2025/homework/">homework</a>
        <a class="page-link" href="/pgm-spring-2025/project/">project</a>
      </div>
    </nav>

  </div>

</header>



    <div class="page-content">

      <d-title>
        <h1>Lecture 03 - A Linear View of Discriminative and Generative Models</h1>
        <p>Introduction to LaTex & Distinction between Discriminative and Generative Models</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <ul>
  <li><a href="#logistics-review"><strong>Logistics Review</strong></a></li>
  <li><a href="#homework-info"><strong>Homework info</strong></a></li>
  <li><a href="#introduction-to-latex"><strong>Introduction to LaTeX</strong></a></li>
  <li><a href="#discriminative-and-generative-models"><strong>Discriminative and Generative Models</strong></a></li>
</ul>

<h2 id="logistics-review">Logistics Review</h2>

<ul>
  <li><strong><a href="https://docs.google.com/spreadsheets/d/1-Mj0MwkSxidVe-HfnMZyUIk4N8cwMeuGzEYTrgDjKqk/edit?gid=0">Lecture scribe sign-up sheet</a></strong></li>
  <li><strong>Instructor</strong>: Ben Lengerich</li>
  <li>Office Hours: Thursday 2:30-3:30pm, 7278 Medical Sciences Center</li>
  <li>Email: <a href="mailto:lengerich@wisc.edu">lengerich@wisc.edu</a></li>
  <li><strong>TA</strong>: Chenyang Jiang</li>
  <li>Office Hours: Monday 11am-12pm, 1219 Medical Sciences Center</li>
  <li>Email: <a href="mailto:cjiang77@wisc.edu">cjiang77@wisc.edu</a></li>
</ul>

<h2 id="homework-info">Homework info</h2>

<ul>
  <li><strong>Released</strong>: Due Friday at midnight.</li>
  <li><strong>Submit via</strong>: <a href="https://canvas.wisc.edu/courses/447453/assignments">Canvas</a></li>
</ul>

<h2 id="introduction-to-latex">Introduction to LaTeX</h2>

<h3 id="what-is-latex">What is LaTeX</h3>
<ul>
  <li>LaTeX is a document preparation system and markup language designed for creating high-quality documents.</li>
  <li><strong>Key Ideas</strong>:
    <ul>
      <li>Separation of content and formatting.</li>
      <li>Not WYSIWYG (like Word); instead, uses precise design commands.</li>
    </ul>
  </li>
  <li><strong>Implications</strong>:
    <ul>
      <li>Handles complex layouts like tables, equations, and citations seamlessly.</li>
      <li>Some upfront work, but no fighting the system for layout.</li>
    </ul>
  </li>
</ul>

<h3 id="why-use-latex">Why Use LaTeX</h3>
<ul>
  <li><strong>Professional Output</strong>: Superior typesetting for academic papers, theses, and technical reports.</li>
  <li><strong>Handles Complexities</strong>: Simplifies math formulas, bibliographies, and cross-referencing.</li>
  <li><strong>Automation</strong>: Automatically generates tables of contents, indices, and citations.</li>
  <li><strong>Reproducibility</strong>: Consistent document appearance across different platforms.</li>
</ul>

<h3 id="basics-of-a-latex-document">Basics of a LaTeX Document</h3>
<ul>
  <li><strong>Preamble</strong>: Sets up document type and packages.
    <div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">\documentclass</span><span class="p">{</span>article<span class="p">}</span>
<span class="k">\usepackage</span><span class="na">[utf8]</span><span class="p">{</span>inputenc<span class="p">}</span>
</code></pre></div>    </div>
  </li>
  <li><strong>Content</strong>: The body of your document.
    <div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">\begin{document}</span>
Hello, World!
<span class="nt">\end{document}</span>
</code></pre></div>    </div>
    <h3 id="essential-commands">Essential Commands</h3>
  </li>
  <li><strong>Text Formatting:</strong>
    <ul>
      <li>Bold: <code class="language-plaintext highlighter-rouge">\textbf{bold text}</code></li>
      <li>Italics: <code class="language-plaintext highlighter-rouge">\textit{italic text}</code></li>
      <li>Underline: <code class="language-plaintext highlighter-rouge">\underline{text}</code></li>
    </ul>
  </li>
  <li><strong>Lists:</strong>
    <div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">\begin{itemize}</span>
<span class="k">\item</span> Item 1
<span class="k">\item</span> Item 2
<span class="nt">\end{itemize}</span>
</code></pre></div>    </div>
    <h3 id="math-in-latex">Math in LaTeX</h3>
  </li>
  <li><strong>In-line math:</strong> Single $ to start/end math environment:<code class="language-plaintext highlighter-rouge">$E = mc^2$</code></li>
  <li><strong>Block math:</strong>
    <div class="language-latex highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">\[</span><span class="nb">
E </span><span class="o">=</span><span class="nb"> mc</span><span class="p">^</span><span class="m">2</span><span class="nb">
</span><span class="p">\]</span>
</code></pre></div>    </div>
    <h3 id="figures">Figures</h3>
  </li>
</ul>
<figure id="example of figures in LaTeX" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/pgm-spring-2025/assets/img/notes/lecture-03/figures.jpg" style="width:80%; max-width:800px;" />
    </div>
  </div>
  </figure>

<h3 id="tables">Tables</h3>
<figure id="example of tables in LaTeX" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/pgm-spring-2025/assets/img/notes/lecture-03/tables.jpg" style="width:70%; max-width:800px;" />
    </div>
  </div>
  </figure>

<h3 id="where-to-use-latex">Where to Use LaTeX</h3>
<ul>
  <li><strong><a href="https://www.overleaf.com/edu/wisc">Overleaf</a></strong>: Recommended for live collaboration and automatic backups.
    <figure id="OverLeaf" class="l-body-outset">
<div class="row">
  <div class="col three">
    <img src="/pgm-spring-2025/assets/img/notes/lecture-03/overleaf.jpg" style="width:50%; max-width:800px;" />
  </div>
</div>
</figure>
  </li>
  <li><strong>Local Editors</strong>: TeXShop, MikTeX, TeXworks.</li>
  <li><strong>Workflow</strong>:
    <ul>
      <li>Write LaTeX code.</li>
      <li>Compile using the “Recompile” button on Overleaf or with <code class="language-plaintext highlighter-rouge">pdflatex</code>/<code class="language-plaintext highlighter-rouge">xelatex</code> locally.</li>
      <li>View and iterate on the generated PDF.</li>
    </ul>
  </li>
</ul>

<h3 id="resources">Resources</h3>
<ul>
  <li><strong><a href="https://www.overleaf.com/learn/latex/Tutorials">Overleaf Tutorials</a></strong></li>
  <li><strong><a href="https://en.wikibooks.org/wiki/LaTeX">LaTeX Wikibook</a></strong></li>
  <li><strong><a href="https://tex.stackexchange.com">TeX Stack Exchange</a></strong> for Q&amp;A</li>
</ul>

<hr />

<h2 id="discriminative-and-generative-models">Discriminative and Generative Models</h2>

<p><strong>Generative Models:</strong> Model the joint distribution, $P(X, Y)$. They aim to model the underlying data distribution 
<strong>Discriminative models:</strong> Models the conditional distribution, $P(Y\mid X)$. They focus on just distinguishing between different classes.</p>
<figure id="generative and discriminative models" class="l-body-outset">
  <div class="row">
    <div class="col three">
      <img src="/pgm-spring-2025/assets/img/notes/lecture-03/disc&amp;gen.jpg" style="width:40%; max-width:800px;" />
    </div>
  </div>
  </figure>

<h3 id="the-discriminative-and-generative-path-to-pymid-x">The discriminative and Generative Path to $P(Y\mid X)$:</h3>
<p>Both Discriminitve and generative models end at $P(Y\mid X)$ but they get there in different ways.</p>

<p><strong>Generative Models</strong> Observe X and Y, then learn $P(X\mid Y)$, $P(Y)$ and calculate $P(X) = \int P(X,Y) \, dY$. Then, use these to calcualate $P(Y\mid X) = \frac{P(X\mid Y)P(Y)}{P(X)}$. 
When looking to calculate $\hat{Y}$, we just need to learn $P(X\mid Y)$ and $P(Y)$. Then we can calculate $\hat{Y} = \arg\max P(X\mid Y) \cdot P(Y)$</p>

<p><strong>Discriminitive Models</strong> Observe X and Y, and then learn $P(Y\mid X)$ . It is a more straightforward approach, but you do not learn as much about X, Y and their distributions.
Similarily, if we are looking to get $\hat{Y}$ from observing X and Y, we can learn $P(X \mid Y)$ and use it calculate $\hat{Y} = \arg\max P(Y\mid X)$</p>

<h3 id="example-discriminative-model---logistic-regression">Example Discriminative Model - Logistic Regression:</h3>
<p>Our goal here is to observe X and Y and from them learn $P(Y\mid X)$. We do this through Parameterization:</p>
<ul>
  <li>$P(Y = 1\mid X) = \sigma(\theta^T X)$, where $\sigma(z) = \frac{1}{1 + e^{-z}}$ is the sigmoid function.</li>
  <li>$P(Y = 0\mid X) = 1 - P(Y = 1\mid X)$</li>
</ul>

<p><strong>Why do we do this parameterization?</strong>
If we take the log-odds and simplify, we get: $\log \left( \frac{P(Y = 1 | X)}{P(Y = 0 | X)} \right) = \theta^T X$ .</p>
<figure id="calculatoin" class="l-body-outset">
    <div class="row">
      <div class="col three">
          <img src="/pgm-spring-2025/assets/img/notes/lecture-03/calculation.jpg" style="width:60%; max-width:800px;" />
      </div>
    </div>
    </figure>
<p>This provides a linear relationship between the input features X and the log-odds of the outcome Y. By using the sigmoid function, we map this linear relationship to the probability space, 
ensuring that the predicted probability lies between 0 and 1.</p>

<p>Using these paramterizations, we can calculate $\hat{\theta}$ from observations of X and Y in our data: 
$\hat{\theta} = \arg\max \prod_{i} P(Y_i \mid X_i; \theta) 
= \arg\max \sum_{i} \left( Y_i \log \sigma(\theta^T X_i) + (1 - Y_i) \log (1 - \sigma(\theta^T X_i)) \right)$</p>

<h3 id="example-generative-model---naive-bayes">Example Generative Model - Naive Bayes:</h3>
<p>Our goal here is to observe X and Y and from there learn $P(X\mid Y)$ &amp; $P(Y)$ and calculate $P(X) = \int P(X, Y) \, dY$. Then, use these to calcualate $P(Y\mid X) = \frac{P(X\mid Y)P(Y)}{P(X)}$.
We do this through Parameterization:</p>
<ul>
  <li>$P(X\mid Y) = \prod_{j=1}^{d} P(X_j\mid Y)$</li>
  <li>$P(Y = k) = \frac{\text{number of samples with Y = k}}{\text{total samples}}$</li>
  <li>$P(X_j\mid Y) = N(\mu_{jk}, \sigma_{jk}^2)$</li>
</ul>

<p><strong>A note on this parameterization?</strong>
This is making the assumption of condtional Independence of the features in $X_1, X_2, \dots, X_n$ given the class label Y. This allows it to simplify the join probability as the product of individual feature 
probabilities, as is shown in the first bullet. This assumption is why it is the “Naive” Bayes.</p>

<p>Using these paramterizations, we can calculate $\hat{\mu}, \hat{\sigma}$ and from observations of X and Y in our data: 
$\hat{\mu}, \hat{\sigma} = \arg\max_{\mu, \sigma} P(X\mid Y)$
$P(Y = 1\mid X) = \frac{\prod_{j=1}^{d} P(X_j\mid Y=1) P(Y=1)}{P(X)}$</p>

<h3 id="what-about-mapregularization">What about MAP/Regularization:</h3>
<p>When we observe X and Y and learn $P(Y\mid X; \sigma)$, $P(\theta)$ in MAP. Our Parameterization stays the same:</p>
<ul>
  <li>$P(Y = 1\mid X) = \sigma(\theta^T X)$, where $\sigma(z) = \frac{1}{1 + e^{-z}}$ is the sigmoid function.</li>
  <li>$P(Y = 0\mid X) = 1 - P(Y = 1\mid X)$</li>
</ul>

<p>From there, we estimate $\hat{\theta}$ from observations: 
$\hat{\theta} = \arg \max \prod_{i} P(Y_i\mid X_i; \theta) P(\theta) = \arg \max_i \sum_{i} \left[ Y_i \log \sigma(\theta^T X_i) + (1 - Y_i) \log(1 - \sigma(\theta^T X_i)) \right] - R(\theta)$</p>

<p>Then, we calaculate $P(Y\mid X)$</p>

<h3 id="discriminitive-vs-generative-models">Discriminitive vs Generative Models:</h3>
<p>Disciminative Models optimize the <em>conditional</em> likelihood:
$\hat{\theta} = \arg\max_{\theta} P(Y\mid X; \theta) = \arg\max_{\theta} \frac{P(X\mid Y; \theta)P(Y; \theta)}{P(X; \theta)}$
Generative Models optimize the <em>joint</em> likelihood:
$\hat{\theta} = \arg\max_{\theta} P(X, Y; \theta) = \arg\max_{\theta} P(X\mid Y; \theta) P(Y; \theta)$</p>

<p><strong>Are these the same optimization?</strong>
Only when $P(X; \theta)$ is invaraiant to $\theta$, as  $\arg\max_{\theta} \frac{P(X | Y; \theta) P(Y; \theta)}{P(X; \theta)} = \arg\max_{\theta} P(X\mid Y; \theta) P(Y; \theta)$</p>

<h3 id="summary-of-logistic-regression-vs-naive-bayes">Summary of Logistic Regression vs Naive Bayes:</h3>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>Logistic Regression</th>
      <th>Naïve Bayes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><strong>Defines</strong></td>
      <td>$P(Y\mid X; \theta)$</td>
      <td>$P(X, Y; \theta)$</td>
    </tr>
    <tr>
      <td><strong>Estimates</strong></td>
      <td>$\hat{\theta} = \arg\max P(Y \mid X; \theta)$</td>
      <td>$\hat{\theta} = \arg\max P(X\mid Y; \theta) P(Y)$</td>
    </tr>
    <tr>
      <td><strong>Asymptotic Error</strong></td>
      <td>Lower asymptotic error on classification</td>
      <td>Higher asymptotic error on classification</td>
    </tr>
    <tr>
      <td><strong>Convergence Speed</strong></td>
      <td>Slower convergence in terms of samples</td>
      <td>Faster convergence in terms of samples</td>
    </tr>
  </tbody>
</table>

<h3 id="andrew-ngs-insights">Andrew Ng’s Insights</h3>

<p>Discriminative learning generally leads to lower error in the long run, but a generative classifier can reach its higher error rate more quickly as it learns. 
In other words, while discriminative models may perform better overall, generative models might be able to learn and converge faster, even if they don’t 
ultimately achieve the same level of accuracy. Why is this?</p>
<figure id="Ng" class="l-body-outset">
    <div class="row">
      <div class="col three">
        <img src="/pgm-spring-2025/assets/img/notes/lecture-03/Ng.jpg" style="width:80%; max-width:800px;" />
      </div>
    </div>
    </figure>
<p>“While discriminative learning has lower asymptotic error, a generative classifier may also approach its (higher) asymptotic error much faster.”
The assumptions of this statement:</p>
<ul>
  <li>Generative models, such as those that estimate $( P(X, Y, \theta) )$, typically make more assumptions to simplify the problem compared to discriminative models that focus on $( P(Y\mid X) )$.</li>
  <li>However, this isn’t always the case.</li>
  <li>Currently, there is no universally correct or general rule for deciding whether to use a discriminative or generative approach to classify an observation $( x )$ into a class $( y )$; the decision depends on how confident we are in the accuracy of the specification of either $( p(y\mid x) )$ or $( p(x, y) )$ for the data.</li>
</ul>

<p><a href="https://link.springer.com/article/10.1007/s11063-008-9088-7">Xue&amp;Tittering 2008</a></p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Introduction to Probabilistic Graphical Models and Modern Probabilistic AI</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Ben Lengerich</li><li><a class="u-email" href="mailto:TBD@wisc.edu">TBD@wisc.edu</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/AdaptInfer" target="_blank"><i class="fab fa-github"></i> <span class="username">AdaptInfer</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>&copy; Copyright 2025 University of Wisconsin. <br />
        Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

  <d-bibliography src="/pgm-spring-2025/assets/bibliography/2025-01-28-lecture-03.bib">
  </d-bibliography>

  <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/pgm-spring-2025/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.1/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.1/katex.min.js"></script>
<script src="/pgm-spring-2025/assets/js/katex.js"></script>



<!-- Load Anchor JS -->
<script src="//cdnjs.cloudflare.com/ajax/libs/anchor-js/3.2.2/anchor.min.js"></script>
<script>
  anchors.options.visible = 'hover';
  anchors.add('article h2, article h3, article h4, article h5, article h6');
</script>



<!-- Adjust LaTeX JS -->
<script src="/pgm-spring-2025/assets/js/latex.js"></script>


<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/pgm-spring-2025/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/pgm-spring-2025/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', '', 'auto');
ga('send', 'pageview');
</script>


</html>
