\documentclass[12pt,letterpaper, onecolumn]{exam}
\usepackage[lmargin=71pt, tmargin=1.2in]{geometry}  %For centering solution box
\lhead{STAT 479\\}
\rhead{HW 1\\}
\thispagestyle{empty}   %For removing header/footer from page 1
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}
\setlength{\topsep}{0pt}
\setlength{\partopsep}{0pt}
\usepackage{enumitem}
\usepackage{titlesec}
\titlespacing{\section}{0pt}{*0.2}{*0.2}


\begin{document}

\begingroup  
    \centering
    \LARGE STAT 479: Homework 1\\[0.5em]
    \large Due: 11:59PM January 31, 2025 by Canvas\\[0.5em]
\endgroup
\rule{\textwidth}{0.4pt}
\pointsdroppedatright   %Self-explanatory
\printanswers
\renewcommand{\solutiontitle}{\noindent\textbf{Answer:}\enspace}


\begin{questions}
\section*{Part 1: Probability Basics}

\question[10 points]\textbf{Conditional Probabilities}\droppoints

Suppose we roll two standard six-sided dice. Let \( A \) be the event ``the sum is greater than 7," and \( B \) be the event ``the first die shows a 4." Select the correct answers for the following probabilities:
\begin{enumerate}[label=(\alph*)]
    \item \( P(A) \):
    \begin{choices}
        \choice \( \frac{5}{12} \)
        \choice \( \frac{7}{12} \)
        \choice \( \frac{1}{6} \)
        \choice \( \frac{1}{2} \)
    \end{choices}
    \item \( P(A \cap B) \):
    \begin{choices}
        \choice \( \frac{1}{6} \)
        \choice \( \frac{1}{12} \)
        \choice \( \frac{5}{36} \)
        \choice \( \frac{1}{4} \)
    \end{choices}
    \item \( P(A | B) \):
    \begin{choices}
        \choice \( \frac{1}{3} \)
        \choice \( \frac{5}{12} \)
        \choice \( \frac{2}{3} \)
        \choice \( \frac{1}{2} \)
    \end{choices}
    \item Are \( A \) and \( B \) independent?
    \begin{choices}
        \choice Yes, \( P(A \cap B) = P(A) \cdot P(B) \).
        \choice No, \( P(A \cap B) \neq P(A) \cdot P(B) \).
    \end{choices}
\end{enumerate}


\begin{solution}
        Write your solution here. For multiple choice questions, only the letter answer is required.
    \begin{parts}
        \part 
        \part 
        \part 
        \part 
    \end{parts}
\end{solution}

%\clearpage
\question[10 points]\textbf{Bayes' Rule}\droppoints

A medical test for a rare disease has the following properties, where $T$ is the test and $D$ is the disease:
\begin{itemize}
    \item \( P(T^+ | D) = 0.95 \),
    \item \( P(T^+ | \neg D) = 0.02 \),
    \item \( P(D) = 0.001 \).
\end{itemize}

\begin{enumerate}[label=(\alph*)]
    \item Using Bayes’ rule, compute \( P(D | T^+) \). Select the correct answer:
    \begin{choices}
        \choice \( 0.32 \)
        \choice \( 0.047 \)
        \choice \( 0.019 \)
        \choice \( 0.001 \)
    \end{choices}
    \item At what $P(D)$ would the test give 95\% confidence?
    \begin{choices}
        \choice $0.10$
        \choice $0.20$
        \choice $0.30$
        \choice $0.50$
    \end{choices}
\end{enumerate}

\begin{solution}
    \begin{parts}
        \part 
        \part 
    \end{parts}
\end{solution}

\question[10 points]\textbf{Continuous Random Variables}\droppoints

The probability density function (PDF) of a continuous random variable \( X \) is:
\[
f(x) =
\begin{cases}
2x & \text{if } 0 \leq x \leq 1, \\
0 & \text{otherwise.}
\end{cases}
\]
\begin{enumerate}[label=(\alph*)]
    \item Is \( f(x) \) a valid PDF? Select the correct answer:
    \begin{choices}
        \choice Yes, \( \int_{-\infty}^\infty f(x) dx = 1 \).
        \choice No, \( f(x) \) does not integrate to 1.
    \end{choices}
    \item Compute \( P(0.25 \leq X \leq 0.75) \). Select the correct answer:
    \begin{choices}
        \choice \( 0.25 \)
        \choice \( 0.50 \)
        \choice \( 0.75 \)
        \choice \( 1.00 \)
    \end{choices}
    \item Determine the expected value \( E[X] \). Select the correct answer:
    \begin{choices}
        \choice \( 0.25 \)
        \choice \( 0.33 \)
        \choice \( 0.50 \)
        \choice \( 0.67 \)
    \end{choices}
        \item Determine the variance of $X$. Select the correct answer:
    \begin{choices}
        \choice \( 0.056 \)
        \choice \( 0.111 \)
        \choice \( 0.167 \)
        \choice \( 0.222 \)
    \end{choices}
\end{enumerate}

\begin{solution}
    \begin{parts}
        \part 
        \part 
        \part 
        \part 
    \end{parts}
\end{solution}


\question[10 points]\textbf{Joint and Marginal Probabilities}\droppoints

Two random variables \( X \) and \( Y \) have the following joint probability mass function (PMF):

\[
P(X = x, Y = y) =
\begin{cases}
\frac{x + y}{12}, & x, y \in \{1, 2\}, \\
0, & \text{otherwise.}
\end{cases}
\]

\begin{enumerate}[label=(\alph*)]
    \item Which property must hold for \( P(X, Y) \) to be a valid joint PMF?
    \begin{choices}
        \choice \( P(X = x, Y = y) \geq 0 \) for all \( x, y \).
        \choice \( \sum_x \sum_y P(X = x, Y = y) = 1 \).
        \choice Both (A) and (B).
        \choice None of the above.
    \end{choices}

    \item Compute the marginal probability \( P(X = 1) \). 
    \begin{choices}
        \choice \( 0.32 \)
        \choice \( 0.35 \)
        \choice \( 0.42 \)
        \choice \( 0.45 \)
    \end{choices}

    \item Compute the conditional probability \( P(Y = 2 \mid X = 2) \).
    \begin{choices}
        \choice \( 0.45 \)
        \choice \( 0.50 \)
        \choice \( 0.57 \)
        \choice \( 0.65 \)
    \end{choices}
\end{enumerate}

\begin{solution}
    \begin{parts}
        \part 
        \part 
        \part 
    \end{parts}
\end{solution}

\clearpage
\section*{Part 2: Estimation}

\question[10 points]\textbf{MLE and MAP for a Possibly Biased Coin}\droppoints

Suppose you are flipping a coin that might be biased (i.e., the probability of heads \( \theta \) is unknown and not necessarily \( 0.5 \)). You flip the coin \( n \) times and observe \( k \) heads.

\begin{enumerate}[label=(\alph*)]
    \item Using the Bernoulli likelihood function, what is the Maximum Likelihood Estimate (MLE) for \( \theta \)?
    \begin{choices}
        \choice \( \frac{n}{k} \)
        \choice \( \frac{k}{n} \)
        \choice \( k \cdot n \)
        \choice \( 1 - \frac{k}{n} \)
    \end{choices}
    \item Suppose you have prior knowledge that the coin is likely close to fair, modeled using a Beta prior \( \theta \sim \text{Beta}(\alpha, \beta) \). The posterior distribution is:
    \[
    \text{Posterior}(\theta | \text{data}) \propto \theta^{k + \alpha - 1} (1 - \theta)^{n - k + \beta - 1}.
    \]

    What is the MAP estimate for \( \theta \)?
    \begin{choices}
        \choice \( \frac{k}{n} \)
        \choice \( \frac{k + \alpha - 1}{n + \alpha + \beta - 2} \)
        \choice \( \frac{k}{n + \alpha + \beta} \)
        \choice \( \frac{k + \alpha}{n + \beta} \)
    \end{choices}
\end{enumerate}

\begin{solution}
    \begin{parts}
        \part 
        \part  
    \end{parts}
\end{solution}



\question[10 points]\textbf{Poisson Distribution}\droppoints

The Poisson distribution models the probability of observing a count \( x_i \) with the rate parameter \( \lambda \):
\[
P(x_i | \lambda) = \frac{\lambda^{x_i} e^{-\lambda}}{x_i!}.
\]

\begin{enumerate}[label=(\alph*)]
    \item Which of the following represents the likelihood function \( L(\lambda) \) for a single observation \( x_1 \)?
    \begin{choices}
        \choice \( \frac{\lambda^{x_1} e^{-\lambda}}{x_1!} \)
        \choice \( \lambda x_1 e^{-\lambda} \)
        \choice \( \lambda^{x_1 - 1} e^{-\lambda} \)
        \choice \( \lambda e^{-x_1} \)
    \end{choices}
    \item Which of the following represents the likelihood function \( L(\lambda) \) for \( n \) independent observations \( x_1, x_2, \ldots, x_n \)?
    \begin{choices}
        \choice \( \prod_{i=1}^n \frac{\lambda^{x_i} e^{-\lambda}}{x_i!} \)
        \choice \( \prod_{i=1}^n \lambda x_i e^{-\lambda} \)
        \choice \( \sum_{i=1}^n \frac{\lambda^{x_i} e^{-\lambda}}{x_i!} \)
        \choice \( \prod_{i=1}^n \lambda^{x_i - 1} e^{-\lambda} \)
    \end{choices}
    \item Which of the following represents the log-likelihood function \( \log L(\lambda) \)?
    \begin{choices}
        \choice \( \sum_{i=1}^n x_i \log \lambda - n\lambda - \sum_{i=1}^n \log(x_i!) \)
        \choice \( n \lambda - \sum_{i=1}^n x_i \log \lambda - \sum_{i=1}^n \log(x_i!) \)
        \choice \( n \log \lambda - n\lambda - \sum_{i=1}^n \log(x_i!) \)
        \choice \( \sum_{i=1}^n x_i \lambda - n\lambda \)
    \end{choices}
    \item What is the MLE for \( \lambda \), the rate parameter?
    \begin{choices}
        \choice \( \frac{\sum_{i=1}^n x_i}{n} \)
        \choice \( \sum_{i=1}^n x_i \)
        \choice \( n \cdot \sum_{i=1}^n x_i \)
        \choice \( \frac{n}{\sum_{i=1}^n x_i} \)
    \end{choices}
\end{enumerate}

\begin{solution}
    \begin{parts}
        \part 
        \part 
        \part 
        \part 
    \end{parts}
\end{solution}


\question[10 points]\textbf{Deriving Backpropagation from MLE}\droppoints

Consider a neural network with one hidden layer. The network’s output is given by:
\[
\hat{y} = \sigma(w_2 \cdot h),
\]
where:
\begin{itemize}
    \item \( h = \sigma(w_1 \cdot x) \),
    \item \( \sigma(z) \) is the sigmoid activation function defined as \( \sigma(z) = \frac{1}{1 + e^{-z}} \),
    \item \( w_1 \) and \( w_2 \) are weights,
    \item \( x \) is the input.
\end{itemize}

Assume that the training data \((x, y)\) are drawn i.i.d. from a distribution, and the network is trained using Maximum Likelihood Estimation (MLE). For binary classification, the likelihood of the data is given by:
\[
P(y | x, w_1, w_2) = \hat{y}^y (1 - \hat{y})^{1-y},
\]
where \( \hat{y} \) is the predicted probability for the positive class.

\begin{enumerate}[label=(\alph*)]
    \item Which of the following represents the negative log-likelihood \( \mathcal{L} \)?
    \begin{choices}
        \choice \( -y \log \hat{y} - (1-y) \log (1-\hat{y}) \)
        \choice \( y \log (1-\hat{y}) + (1-y) \log \hat{y} \)
        \choice \( \hat{y} \cdot y + (1-\hat{y}) \cdot (1-y) \)
        \choice \( y \cdot \hat{y} + (1-y) \cdot (1-\hat{y}) \)
    \end{choices}
    \item What is the gradient of \( \mathcal{L} \) with respect to \( w_2 \)?
    \begin{choices}
        \choice \( (\hat{y} - y) \cdot h \)
        \choice \( (y - \hat{y}) \cdot h \)
        \choice \( \hat{y} \cdot (1 - h) \)
        \choice \( y \cdot (1 - \hat{y}) \)
    \end{choices}
    \item What is the gradient of \( \mathcal{L} \) with respect to \( w_1 \)?
    \begin{choices}
        \choice \( (\hat{y} - y) \cdot w_2 \cdot \sigma'(w_1 \cdot x) \cdot x \)
        \choice \( (\hat{y} - y) \cdot \sigma'(w_1 \cdot x) \cdot x \)
        \choice \( (\hat{y} - y) \cdot w_2 \cdot x \)
        \choice \( (\hat{y} - y) \cdot \sigma'(x) \cdot w_2 \)
    \end{choices}
\end{enumerate}


\begin{solution}
    \begin{parts}
        \part 
        \part 
        \part 
    \end{parts}
\end{solution}


\end{questions}

\end{document}