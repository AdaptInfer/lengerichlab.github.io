\documentclass[12pt,letterpaper, onecolumn]{exam}
\usepackage[lmargin=71pt, tmargin=1.2in]{geometry}  %For centering solution box
\lhead{STAT 479\\}
\rhead{HW 2\\}
\thispagestyle{empty}   %For removing header/footer from page 1
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\setlength{\parindent}{0pt}
\setlength{\parskip}{1em}
\setlength{\topsep}{0pt}
\setlength{\partopsep}{0pt}
\usepackage{enumitem}
\usepackage{titlesec}
\titlespacing{\section}{0pt}{*0.2}{*0.2}


\begin{document}

\begingroup
    \centering
    \LARGE STAT 479: Homework 2\\[0.5em]
    \large Due: 11:59PM Feb 11, 2025 by Canvas\\[0.5em]
\endgroup
\rule{\textwidth}{0.4pt}
\pointsdroppedatright   %Self-explanatory
\printanswers
\renewcommand{\solutiontitle}{\noindent\textbf{Answer:}\enspace}


\begin{questions}

\section*{Part 1: Naive Bayes}

\question[20 points]\textbf{MLE for Gaussian Naive Bayes}\droppoints

Naive Bayes assumes that features \( X = (X_1, X_2, \dots, X_d) \) are conditionally independent given the class \( Y \), and that \( X_i | Y \sim \mathcal{N}(\mu_{i,Y}, \sigma_{i,Y}^2) \).

\begin{itemize}
    \item Features are conditionally independent given the class:
    \[
    P(X|Y) = \prod_{i=1}^d P(X_i|Y).
    \]
    \item Each feature \( X_i|Y=k \) follows a Gaussian distribution:
    \[
    P(X_i|Y=k) = \frac{1}{\sqrt{2\pi \sigma_{i,k}^2}} \exp\left(-\frac{(X_i - \mu_{i,k})^2}{2\sigma_{i,k}^2}\right).
    \]
\end{itemize}

\begin{enumerate}[label=(\alph*)]
    \item \textbf{Likelihood Function}: Write the likelihood of the observed data \( \{X_{i,j}\}_{j : Y_j = k} \), assuming \( X_i|Y=k \) is Gaussian. Select the correct expression:
    \begin{choices}
        \choice \( \prod_{j : Y_j=k} \prod_{i=1}^d P(X_{i,j}|Y=k) \)
        \choice \( \prod_{j : Y_j=k} \prod_{i=1}^d \frac{1}{2\pi \sigma_{i,k}^2} \exp\left(-\frac{(X_{i,j} - \mu_{i,k})^2}{\sigma_{i,k}^2}\right) \)
        \choice \( \prod_{j : Y_j=k} \prod_{i=1}^d \frac{1}{\sqrt{2\pi \sigma_{i,k}^2}} \exp\left(-\frac{(X_{i,j} - \mu_{i,k})^2}{2\sigma_{i,k}^2}\right) \)
        \choice \( \prod_{j : Y_j=k} \prod_{i=1}^d P(Y=k) \cdot P(X_{i,j}) \)
    \end{choices}

    \item \textbf{Log-Likelihood}: Write the log-likelihood for \( \mu_{i,k} \) and \( \sigma_{i,k}^2 \). For notation convenience, let $N_k$ be the number of samples with label $k$: $N_k = \sum_{j=1}^n \mathbb{I}(Y_j=k)$ . Select the correct expression:
    \begin{choices}
        \choice \( -\frac{N_k}{2} \log(2\pi \sigma_{i,k}^2) - \frac{1}{2\sigma_{i,k}^2} \sum_{j : Y_j=k} (X_{i,j} - \mu_{i,k})^2 \)
        \choice \( -\frac{1}{2} \log(2\pi \sigma_{i,k}^2) - \frac{1}{\sigma_{i,k}^2} \sum_{j : Y_j=k} (X_{i,j} - \mu_{i,k})^2 \)
        \choice \( \sum_{j : Y_j=k} \log P(X_{i,j}|Y=k) + \log P(Y=k) \)
        \choice \( -\frac{N_k}{2} \log(2\pi) + \frac{\sum_{j : Y_j=k} (X_{i,j} - \mu_{i,k})^2}{2\sigma_{i,k}^2} \)
    \end{choices}

    \item \textbf{MLE for \( \mu_{i,k} \)}: Derive the MLE for \( \mu_{i,k} \). Select the correct estimator:
    \begin{choices}
        \choice \( \widehat{\mu_{i,k}} = \frac{\sum_{j : Y_j=k} X_{i,j}}{N_k} \)
        \choice \( \widehat{\mu_{i,k}} = \frac{\sum_{j=1}^n X_{i,j}}{n} \)
        \choice \( \widehat{\mu_{i,k}} = \frac{\sum_{j : Y_j=k} X_{i,j}^2}{N_k} \)
        \choice \( \widehat{\mu_{i,k}} = \sum_{j : Y_j=k} X_{i,j} \)
    \end{choices}
\end{enumerate}

\begin{solution}
    \begin{parts}
        \part
        \part
        \part
    \end{parts}
\end{solution}

\question[20 points]\textbf{Modified Naive Bayes with Feature Dependencies}\droppoints

\paragraph*{Background:}
In the standard Naive Bayes classifier, we assume that all features \( X_1, X_2, \dots, X_d \) are conditionally independent given the class label \( Y \):

\[
P(X_1, X_2, \dots, X_d \mid Y) = \prod_{i=1}^{d} P(X_i \mid Y)
\]

However, in many real-world scenarios, features exhibit dependencies. To address this, we modify the Naive Bayes model to account for such dependencies. Specifically, we introduce a structure where some features are conditionally dependent on others.

\paragraph*{Problem Statement:}
Consider a dataset with four features \( X_1, X_2, X_3, X_4 \) and a binary class label \( Y \in \{0,1\} \). Instead of assuming full independence, we impose the dependency structure captured in the following joint distribution:

\[
P(X_1, X_2, X_3, X_4 \mid Y) = P(X_1 \mid Y) P(X_2, X_3 \mid X_1, Y) P(X_4 \mid Y)
\]

Here, \( P(X_2, X_3 \mid X_1, Y) \) is modeled as a joint Gaussian distribution with mean and covariance matrix dependent on \( Y \).

Given the probability distributions:
\begin{itemize}
    \item $P(Y=0) = 0.5, P(Y=1) = 0.5$
    \item \( P(X_1 \mid Y) \) and \( P(X_4 \mid Y) \) are \textbf{standard normal} univariate Gaussians.
    \item \( P(X_2, X_3 \mid X_1, Y) \) follows a multivariate Gaussian distribution:
    \begin{itemize}
        \item \textbf{For} \( Y = 0 \):
        \[
        \mu_{X_2, X_3} = \begin{bmatrix} 2 \\ 3 \end{bmatrix} + \alpha X_1, \quad \Sigma_{X_2, X_3} = \begin{bmatrix} 1 & 0.5 \\ 0.5 & 1 \end{bmatrix}
        \]
        \item \textbf{For} \( Y = 1 \):
        \[
        \mu_{X_2, X_3} = \begin{bmatrix} 1 \\ -1 \end{bmatrix} + \alpha X_1, \quad \Sigma_{X_2, X_3} = \begin{bmatrix} 1 & -0.3 \\ -0.3 & 1.5 \end{bmatrix}
        \]
    \end{itemize}
\end{itemize}

\begin{enumerate}[label=(\alph*)]
    \item \textbf{Computing \( P(X \mid Y) \)} \\
What is \( P(X_1=0, X_2=2, X_3=3, X_4=0 \mid Y=0)\) under this modified Naive Bayes model?
    \begin{choices}
        \choice 0.001
        \choice 0.01
        \choice 0.03
        \choice 0.3
    \end{choices}
    \item \textbf{Computing \( P(X, Y) \)} \\
What is \( P(X_1=0, X_2=2, X_3=3, X_4=0, Y=0)\) under this modified Naive Bayes model?
    \begin{choices}
        \choice 0.001
        \choice 0.015
        \choice 0.2
        \choice 0.5
\end{choices}

    \item \textbf{Model Complexity}
How many parameters are required to fully parameterize this modified Naive Bayes model compared to the standard Naive Bayes model?
\begin{choices}
    \choice \textbf{More than standard Naive Bayes} – because the dependency structure introduces additional covariance terms in \( P(X_2, X_3 \mid X_1, Y) \).
    \choice \textbf{Fewer than standard Naive Bayes} – because the dependency structure reduces the total number of independent conditional distributions.
    \choice \textbf{The same as standard Naive Bayes} – because each feature still depends on the class label \( Y \).
    \choice \textbf{It depends on the dataset} – the number of parameters cannot be determined without knowing the data distribution.
\end{choices}
\end{enumerate}


\begin{solution}
    \begin{parts}
        \part
        \part
        \part
    \end{parts}
\end{solution}


\clearpage
\section*{Part 2: Bayesian Networks}
\question[20 points]\textbf{Conditional Independence and D-Separation}\droppoints

Consider the following Bayesian Network, where \( A \), \( B \), \( C \), and \( D \) are random variables:

\[
A \to B \to D, \quad A \to C \to D.
\]

\begin{enumerate}[label=(\alph*)]
    \item \textbf{Markov Property}: Which of the following statements about conditional independence in this Bayesian Network is correct?
    \begin{choices}
        \choice \( A \) and \( D \) are conditionally independent given \( B \).
        \choice \( A \) and \( D \) are conditionally independent given \( C \).
        \choice \( B \) and \( C \) are conditionally independent given \( D \).
        \choice \( B \) and \( C \) are conditionally independent given \( A \).
    \end{choices}

    \item \textbf{Joint Distribution}: Which of the following correctly represents the joint probability \( P(A, B, C, D) \)?
    \begin{choices}
        \choice \( P(A, B, C, D) = P(A)P(B|A)P(C|A)P(D|B, C) \)
        \choice \( P(A, B, C, D) = P(A)P(B)P(C|A)P(D|B, C) \)
        \choice \( P(A, B, C, D) = P(A)P(B|A)P(C)P(D|B, C) \)
        \choice \( P(A, B, C, D) = P(A|B)P(B|C)P(C|D)P(D) \)
    \end{choices}

    \item \textbf{d-separation}: Which of the following pairs of variables are d-separated in the given network, assuming no evidence is observed?
    \begin{choices}
        \choice \( A \) and \( D \)
        \choice \( B \) and \( C \)
        \choice \( A \) and \( C \)
        \choice None of the above
        \choice All of the above
    \end{choices}
    
    \item \textbf{d-separation}: Which of the following pairs of variables are d-separated in the given network, assuming $B$ is observed?
    \begin{choices}
        \choice \( A \) and \( D \)
        \choice \( B \) and \( C \)
        \choice \( A \) and \( C \)
        \choice None of the above
        \choice All of the above
    \end{choices}
        
    \item \textbf{D-Separation and Deep Generative Models:} Suppose a deep generative model learns latent variables $Z$ that mediate dependencies between observed variables. Which of the following statements is true?
    \begin{enumerate}
        \item If $Z$ explains the correlation between $X$ and $Y$, then conditioning on $Z$ should make $X$ and $Y$ independent.
        \item Adding a latent variable always increases dependencies between observed variables.
        \item If $Z$ is a common parent of $X$ and $Y$, then $X$ and $Y$ are always independent.
        \item If $Z$ is observed, it has no effect on the conditional independence structure of $X$ and $Y$.
    \end{enumerate}
\end{enumerate}

\begin{solution}
    \begin{parts}
        \part
        \part
        \part
        \part
        \part
    \end{parts}
\end{solution}


\clearpage
\section*{Part 3: HMM}
\question[20 points]\textbf{HMM Theory}\droppoints

Recall our discussion of \textit{Hidden Markov Models (HMMs)}, which are statistical models where an \textbf{unobserved (hidden) state sequence} influences an \textbf{observed sequence} of data.

\begin{enumerate}[label=(\alph*)]
    \item \textbf{Long-Term Behavior:} If an HMM runs for many timesteps, we expect the probabilities of being in each state to:
\begin{choices}
    \choice Continue changing randomly with no pattern.
    \choice Settle into stable values over time.
    \choice Eventually become equal for all states.
    \choice Always remain the same as the initial probabilities.
\end{choices}
    \item \textbf{Interpretation of \( (P^n)_{ij} \):} Let \( P \) be the matrix of transition probabilities. The value of \( (P^n)_{ij} \) in the transition matrix raised to the power \( n \) represents:
\begin{choices}
    \choice The probability of being in state \( j \) after \( n \) steps, regardless of the initial state.
    \choice The probability of transitioning from state \( i \) to state \( j \) in exactly \( n \) steps.
    \choice The long-run proportion of time spent in state \( j \).
    \choice The expected number of transitions from \( i \) to \( j \) over \( n \) steps.
\end{choices}

\textbf{Example:}
Consider the transition matrix:
\[
P =
\begin{bmatrix}
0.7 & 0.3 \\
0.4 & 0.6
\end{bmatrix}
\]
Then \( P^2 \) gives:
\[
P^2 =
\begin{bmatrix}
0.7 & 0.3 \\
0.4 & 0.6
\end{bmatrix}
\times
\begin{bmatrix}
0.7 & 0.3 \\
0.4 & 0.6
\end{bmatrix}
=
\begin{bmatrix}
0.61 & 0.39 \\
0.52 & 0.48
\end{bmatrix}
\]
where \( (P^2)_{12} = 0.39 \).

\item \textbf{Powers of the Transition Matrix:} The matrix \( P^n \), representing the transition probabilities after \( n \) steps, has the following property:
\begin{choices}
    \choice It becomes a diagonal matrix as \( n \) increases.
    \choice It converges to a matrix where all rows are identical.
    \choice It remains the same as \( P \) for all \( n \).
    \choice It fluctuates indefinitely without a pattern.
\end{choices}

\textit{Hint: In many Markov chains, as \( n \) grows, the transition probabilities stabilize, meaning that each row of \( P^n \) approaches a unique limiting distribution (the stationary distribution). This suggests that long-run behavior is independent of the initial state.}

\textbf{Example:}
For the transition matrix \( P \) above, as \( n \to \infty \), \( P^n \) converges to:
\[
P^\infty \approx
\begin{bmatrix}
0.57 & 0.43 \\
0.57 & 0.43
\end{bmatrix}
\]

\end{enumerate}

\begin{solution}
    \begin{parts}
        \part
        \part
        \part
    \end{parts}
\end{solution}


\question[20 points]\textbf{The Forward-Backward Algorithm}\droppoints

A key question in HMMs is how to compute the probability of a hidden state at a particular time step given all observations.

To do this efficiently, we use the \textbf{Forward-Backward Algorithm}, which introduces two key quantities:

- The \textbf{forward probability}:
  \[
  \alpha_t(Z_t) = P(X_1, \dots, X_t, Z_t)
  \]
  which represents the probability of the first \( t \) observations and the current state.

- The \textbf{backward probability}:
  \[
  \beta_t(Z_t) = P(X_{t+1}, \dots, X_n | Z_t)
  \]
  which represents the probability of future observations given the current state.

Using these definitions, answer the following:

\begin{enumerate}[label=(\alph*)]
    \item \textbf{Recursive Formula for \( \alpha_t(Z_t) \)}
    The forward probability \( \alpha_t(Z_t) \) can be computed recursively using the previous timestep \( \alpha_{t-1}(Z_{t-1}) \):

\begin{choices}
    \choice \( \alpha_t(Z_t) = P(X_t | Z_t) \sum_{Z_{t-1}} P(Z_t | Z_{t-1}) \alpha_{t-1}(Z_{t-1}) \)
    \choice \( \alpha_t(Z_t) = P(X_t | Z_t) P(Z_t) \)
    \choice \( \alpha_t(Z_t) = \sum_{Z_{t-1}} P(X_t | Z_t) P(Z_t | Z_{t-1}) \alpha_{t-1}(Z_{t-1}) \beta_{t+1}(Z_t) \)
    \choice \( \alpha_t(Z_t) = P(X_t | Z_t) P(Z_t | Z_{t-1}) \)
\end{choices}

    \item \textbf{Recursive Formula for \( \beta_t(Z_t) \)}
    Similarly, the backward probability \( \beta_t(Z_t) \) can be computed recursively from the next timestep \( \beta_{t+1}(Z_{t+1}) \):

\begin{choices}
    \choice \( \beta_t(Z_t) = \sum_{Z_{t+1}} P(Z_{t+1} | Z_t) P(X_{t+1} | Z_{t+1}) \beta_{t+1}(Z_{t+1}) \)
    \choice \( \beta_t(Z_t) = P(X_{t+1} | Z_t) P(Z_{t+1} | Z_t) \beta_{t+1}(Z_{t+1}) \)
    \choice \( \beta_t(Z_t) = \sum_{Z_{t+1}} P(Z_t | Z_{t+1}) P(X_{t+1} | Z_t) \beta_{t+1}(Z_{t+1}) \)
    \choice \( \beta_t(Z_t) = P(Z_{t+1} | Z_t) P(X_t | Z_t) \beta_t(Z_t) \)
\end{choices}

    \item \textbf{Computing \( P(Z_t | X) \)}
    Now, using \( \alpha_t(Z_t) \) and \( \beta_t(Z_t) \), the posterior probability of \( Z_t \) given the full sequence \( X \) is:

\begin{choices}
    \choice \( P(Z_t | X) = \frac{\alpha_t(Z_t) \beta_t(Z_t)}{P(X)} \)
    \choice \( P(Z_t | X) = \frac{P(X | Z_t) P(Z_t)}{P(X)} \)
    \choice \( P(Z_t | X) = \frac{P(X_t | Z_t) P(Z_t | X_{1:t-1})}{P(X_t | X_{1:t-1})} \)
    \choice \( P(Z_t | X) = \frac{\alpha_t(Z_t) P(X)}{\beta_t(Z_t)} \)
\end{choices}

    \item \textbf{Why is the Forward-Backward Algorithm Efficient?}
    The Forward-Backward algorithm provides an efficient way to compute \( P(Z_t | X) \) for all time steps \( t \). Which of the following best explains how it reduces computation compared to a naive approach?

\begin{choices}
    \choice Instead of summing over all possible hidden state sequences, it breaks the problem into two recursive computations—one moving forward in time, and one moving backward—reducing the complexity from \( O(n \cdot |Z|^n) \) to \( O(n |Z|^2) \).
    \choice It replaces probabilities with log-probabilities, converting multiplication into addition, which reduces computational cost.
    \choice It precomputes \( P(X) \), avoiding the need for normalization in the final computation of \( P(Z_t | X) \).
    \choice It finds the most probable sequence of hidden states using dynamic programming, rather than computing marginal probabilities for each state individually.
\end{choices}

\end{enumerate}

\begin{solution}
    \begin{parts}
        \part
        \part
        \part
        \part
    \end{parts}
\end{solution}


\end{questions}
\end{document}