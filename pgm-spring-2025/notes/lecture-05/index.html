<!DOCTYPE html>
<html>
  <head>
    <head>
  <meta charset="UTF-8">
  <meta http-equiv="content-language" content="en">
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>STAT 479 PGMs | Lecture 05 - Undirected GMs (MRFs)</title>
  <meta name="description" content="10-708 - Probabilistic Graphical Models - University of Wisconsin-Madison - Spring 2025
">

  <link rel="shortcut icon" href="/pgm-spring-2025/assets/img/favicon.ico">

  <link rel="stylesheet" href="/pgm-spring-2025/assets/css/main.css">
  <link rel="canonical" href="/pgm-spring-2025/notes/lecture-05/">

  
  <!-- Load Latex JS -->
  <script src="https://cdn.jsdelivr.net/npm/latex.js@0.11.1/dist/latex.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/latex.js@0.11.1/dist/latex.component.js"></script>
  
</head>

    <script src="/pgm-spring-2025/assets/js/distillpub/template.v2.js"></script>
    <script src="/pgm-spring-2025/assets/js/distillpub/transforms.v2.js"></script>
  </head>

  <d-front-matter>
    <script type="text/json">{
      "title": "Lecture 05 - Undirected GMs (MRFs)",
      "description": "Introduction to Undirected GMs",
      "published": "February 4, 2025",
      "lecturers": [
        
        {
          "lecturer": "Ben Lengerich",
          "lecturerURL": "https://lengerichlab.github.io/"
        }
        
      ],
      "authors": [
        
        {
          "author": "Justina Jing"
        },
        
        {
          "author": "Lois Liu"
        }
        
      ],
      "editors": [
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body>

    <header class="site-header">

  <div class="wrapper">

    <span class="site-title">
      <a class="page-link" href="https://adaptinfer.org/pgm-spring-2025/">STAT 479 PGMs</a>
    </span>

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <a class="page-link" href="/pgm-spring-2025/logistics/">logistics</a>
        <a class="page-link" href="/pgm-spring-2025/lectures/">lectures</a>
        <a class="page-link" href="/pgm-spring-2025/notes/">notes</a>
        <a class="page-link" href="/pgm-spring-2025/calendar/">calendar</a>
        <a class="page-link" href="/pgm-spring-2025/homework/">homework</a>
        <a class="page-link" href="/pgm-spring-2025/project/">project</a>
      </div>
    </nav>

  </div>

</header>



    <div class="page-content">

      <d-title>
        <h1>Lecture 05 - Undirected GMs (MRFs)</h1>
        <p>Introduction to Undirected GMs</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <h2 id="logistics-review">Logistics Review</h2>
<p><strong>No class on 2/11</strong></p>

<p><strong>HW2 deadline pushed to 2/11 11:59pm</strong></p>

<p><strong>Quiz in-class on 2/13</strong></p>

<p><strong>3 HW problems, 2 new problems. All of the questions are multiple choice. Time: Whole lecture.</strong></p>

<h2 id="undirected-graphic-models">Undirected Graphic Models</h2>
<p><strong>Definition</strong>: UGMs provide a visual representation of the structure of joint probability distribution from which conditional independence relationships can be inferred.</p>
<ul>
  <li>Unlike a directed graphic mode, UGMs <strong>do not imply a causal direction</strong> between the variables. UGMs give correlations between variables. (Figure 1)</li>
  <li>key features of UGMs:
    <ul>
      <li>Pairwise relationship</li>
      <li>no explicit way to generate samples</li>
      <li>Contingency constraints on node configurations.
<img src="image.png" alt="Figure 1: Undirected Graphic Model" />
figure 1</li>
    </ul>
  </li>
  <li>UGM Example: Lattice
    <ul>
      <li>An UGM lattice used for spatial inference, each pixel in the image is represented as a node, each node is connected to its neighboring pixels (Figure 2)
        <ul>
          <li>To infer the yellow pixel, the model classifies the yellow selected pixel based on its surrounding pixels instead of its absolute location.
Since the most of the adjacent pixels belong to “water”, even it’s on the top of right, lattice UGM classifies it as <strong>water</strong>
<img src="image-1.png" alt="Figure 2: Lattice example" />
figure 2</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h2 id="representing-undirected-graphic-model">Representing Undirected Graphic Model</h2>
<ul>
  <li>
    <p>An undirected graphical model represents a distribution $P(x)$ defined by an undirected graph $H$ and a set of positive potential functions $\psi$ associated with the cliques of $H$ such that  $P(X_1,…,X_n) = \frac{1}{Z} \prod_c \psi_c (X_c)$</p>

    <ul>
      <li>$Z$ is the <strong>partition function</strong> (normalization):  $Z = \sum_X \prod_c \psi_c (X_c) $</li>
    </ul>
  </li>
  <li>The potential function $\psi_c(X_c)$ can be understood as a “score” of the joint configuration.</li>
  <li>Unlike probability densities, which must integrate (or sum) to 1, the potential function only provides a score of how favorable a particular configuration is compared to others.</li>
  <li>The ${P(X)}$ is a proper probability density because the partition function is finite, which means that ${P(X)}$ is properly normalized so that sum of all probabilities equals 1. Also, all potential functions are positive.</li>
</ul>

<h2 id="clique">Clique</h2>
<p><strong>Definition</strong>: A clique is a fully connected subset of nodes in an undirected graph model.
for $G = {V,E}$, a clique is a subgraph such that the nodes in $V’$ are fully connected.
<img src="image-2.png" alt="figure 3: Clique" /> Figure 3<br />
Maximal Clique is a clique that cannot be extended by adding more nodes without breaking the full connectivity condition.</p>
<ul>
  <li>${x_1,x_4}$ and ${x_1,x_2,x_3}$ are <strong>maximal clique</strong>.</li>
  <li>${x_1,x_2,x_3}$ clique are fully connected since each node has edges to the other two. Adding $x_4$ would break the clique condition because $x_4$ is not connected to both $x_1$ and $x_2$.</li>
  <li>${x_1,x_2}$, ${x_2,x_3}$, ${x_1,x_3}$, ${x_1}$,${x_2}$, ${x_3}$${x_4}$ are <strong>sub-cliques</strong> because their nodes are directly connected.</li>
</ul>

<p><br />
<strong>Interpretation of Clique Potentials</strong>
<img src="image-3.png" alt="figure 4" />&lt;figcaption&gt; Figure 4</p>
<ul>
  <li>
    <p>The figure 4 model implies $ x_1 \perp x_2 \mid x_3$, so joint must factorize accordingly as:
 $P(X_1,X_2,X_3) = P(X_2) P(X1\mid X2) P(X_3\mid X_2)$</p>
  </li>
  <li>
    <p>In UGMs, we do not use probabilities, we define the distribution in terms of clique potentials $\psi$, which measure\textbf{ “compatibility” }rather than probability. Therefore, we could write the above equation as
 $P(X_1, X_2, X_3) = P(X_1, X_2) P(X_3 \mid X_2)$ or $P(X_3, X_2) P(X_1 \mid X_2)$</p>
    <ul>
      <li><strong>Cannot have all potential be marginals</strong></li>
      <li><strong>Cannot have all potential be conditions</strong></li>
    </ul>
  </li>
</ul>

<p><img src="image-4.png" alt="alt text" />
 figure 5</p>
<ul>
  <li>Maximal Cliques: Based on Figure 5, the probability distribution of the entire system is written as
  $P(A.B,C,D) = \frac{1}{Z}\psi_{ABC}(A, B, C) \psi_{BCD}(B, C, D)$
    <ul>
      <li>To make $P(A,B,C,D)$ a valid probability distribution, we normalize it by using the partition function $Z$.
 Therefore,  $Z = \sum_{ABCD}\psi_{ABC}(A, B, C) \psi_{BCD} (B, C, D) $ This ensures the sum of all possible configurations equals 1.</li>
    </ul>
  </li>
</ul>

<h2 id="markov-independencies">Markov Independencies</h2>
<p><strong>Global Markov Independencies</strong>
!<img src="image-6.png" alt="figure 6" />
Figure 6</p>
<ul>
  <li>The undirected graph $H$ in Graph 6 demonstrates that the nodes in $B$ separate $A$ and $C$, meaning that all paths from any node in $A$ to any node in $C$ must pass through $B$. This is written as $(sep_H(A;C\mid B))$</li>
  <li>If you know the values of $B$, then $A$ provides no additional information about $C$ because all paths are blacked by $B$.</li>
  <li>Global Markov Property: A probability distribution satisfies it if for any disjoint$\ A,B,C $\ such that $B$ separates $A$ and $C$, $A$ is independent of $C$ given $B$. This is written as
$I(H) = {A\perp C \mid B:sep_H(A;C \mid B)}$</li>
</ul>

<p><strong>Local Markov Independencies</strong>
<img src="image-7.png" alt="graph 7" />
Figure 7</p>
<ul>
  <li>In the Graph 7, we make blue node be $X_i$. We say that there is a unique Markov blanket of $X_i$, denoted $M B_{x_i}$, which is the set of neighbors of $X_i$ in the graph 6 (red nodes)</li>
  <li>The local Markov independencies in $H$ are:  $I_l(H) ={X_i\perp V-{X_i}-M B_{x_i}\mid MB_{X_i}:\forall_i}$</li>
  <li>In other words, $X_i$ is independent of the rest of the nodes in the graph given its immediate neighbors, and this is satisfied for all nodes $i$ in the graph. Once you know the Markov blanket, you do not need any other nodes to determine $X_i$</li>
</ul>

<p><strong>Pairwise Markov Independencies</strong>
<img src="image-9.png" alt="figure 8: pairwise" />
figure 8</p>
<ul>
  <li>The pairwise Markov property associated with H are:
 $I_p(H) = {X\perp Y \mid V- {X,Y} : {X,Y} \notin E}$</li>
  <li>In figure 8, we can see that nodes $X_1$ and $X_5$ are not directly connected. By the pairwise Markov independence rule, we can say: $X_1\perp X_5 \mid {X_2,X_3,X_4}$</li>
  <li>This means that once we know $X_2$,$X_3$,and $X_4$, knowing $X_1$ tells us nothing more about $X_5$, because the intermediate nodes block all paths between $X_1$ and $X_5$, ensuring their independence.</li>
</ul>

<h2 id="i-maps-of-ug">I-Maps of UG</h2>
<h3 id="definition-of-i-map">Definition of I-Map</h3>
<p>An undirected graph ( H ) is an <strong>I-Map (Independence Map)</strong> for a probability distribution ( P ) if:</p>

\[I(H) \subseteq I(P)\]

<p>where:</p>
<d-footnote>- \( I(H) \) is the set of independence relationships implied by the graph \( H \).
<d-footnote>- \( I(P) \) is the set of independence relationships present in \( P \).

## Gibbs Distribution
A probability distribution \( P \) is a **Gibbs Distribution** on a graph \( H \) if:

$$
P(X) = \frac{1}{Z} \prod_{c \in C} \psi_c(X_c)
$$

- This means that a **Gibbs distribution** can be decomposed into a product of factors over the cliques of the graph.

### **Theorem:**
If \( P \) is a Gibbs distribution over the graph \( H \), then \( H \) is an **I-Map** of \( P \).

---

## Perfect Maps
<figure id="perfect-map" class="l-body-outset">
  <img src="assets/img/perfect_map.png" />
  <figcaption>
    <strong>Figure 1:</strong> Not all structures have a perfect map.
  </figcaption>
</figure>

### Definition of Perfect Map
An undirected graph \( H \) is a **Perfect Map** for a probability distribution \( P \) if:

$$
\text{sep}_H(X; Z \mid Y) \iff X \perp Z \mid Y
$$

### **Key Point:**
- Not all distributions have a **perfect map** as a UG.
- **Example: V-Structure \( X \to Y \leftarrow Z \)**:
  - \( X \perp Z \)
  - But \( X \) and \( Z \) are **dependent given \( Y \)**:

$$
\neg (X \perp Z \mid Y)
$$

Since an **undirected graph cannot represent directionality**, it cannot capture this dependency.

---

## Graphical Models and Overlapping Distributions
<figure id="gm-overlap" class="l-body-outset">
  <img src="assets/img/overlapping_dists.png" />
  <figcaption>
    <strong>Figure 2:</strong> Overlapping sets of distributions in Directed and Undirected Graphical Models.
  </figcaption>
</figure>

<d-footnote>- **\( P \)**: The largest set, containing all possible probability distributions.
<d-footnote>- **\( D \)**: The set of probability distributions that can be represented by **Directed Graphical Models (DGMs)**, such as Bayesian Networks.
<d-footnote>- **\( U \)**: The set of probability distributions that can be represented by **Undirected Graphical Models (UGMs)**, such as Markov Random Fields.
- The **overlap** represents distributions that can be represented by **both** DGMs and UGMs.

---

## Exponential Families in UGMs
### **Energy Representation**
Clique potentials \( \psi_c(X_c) \) can be rewritten using an **energy function**:

$$
\psi_c(X_c) = \exp(-\phi_c(X_c))
$$

This leads to the **Boltzmann Distribution**:

$$
P(X) = \frac{1}{Z} \exp\left(-\sum_{c \in C} \phi_c(X_c)\right)
$$

- In **physics**, this is called the **Boltzmann distribution**.
- In **statistics**, this is known as a **log-linear model**.

---

## MAP Inference as Free Energy Minimization
### **Boltzmann Distribution**
The **Boltzmann distribution** is defined as:

$$
P_{\text{Boltzmann}}(X) = \arg\min_H F(P(X; H))
$$

where:
- \( P(X;H) \) is the probability distribution over configurations \( X \).
- \( F(P) \) is the **free energy**.

### **Free Energy Decomposition**
$$
F(P) = E[H(X)] - TS(P(X))
$$

where:
- \( E[H(X)] \) is the **expected energy**.
- \( TS(P(X)) \) represents the entropy term.

### **MAP Inference and Free Energy**
In probabilistic inference, **MAP estimation** minimizes the free energy:

$$
Q^*(\theta) = \arg\min_{Q} \left[ E_Q(-\log P(X \mid \theta)) - TS(Q) \right]
$$

- The first term represents the **negative log-likelihood**.
- The second term **regularizes** the distribution.

---

## **Conclusion**
- **I-Maps** and **Gibbs distributions** define the relationship between UGMs and probability distributions.
- **Perfect Maps** do not always exist due to **directionality limitations** in UGMs.
- **Boltzmann distributions** provide an energy-based formulation of UGMs.

---

## **Next Steps**
- Further explore **Variational Inference**.
- Applications of UGMs in **machine learning and physics**.
</d-footnote></d-footnote></d-footnote></d-footnote></d-footnote>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Introduction to Probabilistic Graphical Models and Modern Probabilistic AI</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Ben Lengerich</li><li><a class="u-email" href="mailto:TBD@wisc.edu">TBD@wisc.edu</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/AdaptInfer" target="_blank"><i class="fab fa-github"></i> <span class="username">AdaptInfer</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>&copy; Copyright 2025 University of Wisconsin. <br />
        Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

  <d-bibliography src="/pgm-spring-2025/assets/bibliography/2025-02-04-lecture-05.bib">
  </d-bibliography>

  <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/pgm-spring-2025/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.1/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.1/katex.min.js"></script>
<script src="/pgm-spring-2025/assets/js/katex.js"></script>



<!-- Load Anchor JS -->
<script src="//cdnjs.cloudflare.com/ajax/libs/anchor-js/3.2.2/anchor.min.js"></script>
<script>
  anchors.options.visible = 'hover';
  anchors.add('article h2, article h3, article h4, article h5, article h6');
</script>



<!-- Adjust LaTeX JS -->
<script src="/pgm-spring-2025/assets/js/latex.js"></script>


<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/pgm-spring-2025/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/pgm-spring-2025/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', '', 'auto');
ga('send', 'pageview');
</script>


</html>
