<!DOCTYPE html>
<html>
  <head>
    <head>
  <meta charset="UTF-8">
  <meta http-equiv="content-language" content="en">
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>STAT 479 PGMs | Lecture 20-LLMs from a Probabilistic Perspective 1-Implementing a GPT from Scratch</title>
  <meta name="description" content="10-708 - Probabilistic Graphical Models - University of Wisconsin-Madison - Spring 2025
">

  <link rel="shortcut icon" href="/pgm-spring-2025/assets/img/favicon.ico">

  <link rel="stylesheet" href="/pgm-spring-2025/assets/css/main.css">
  <link rel="canonical" href="/pgm-spring-2025/notes/lecture-20/">

  
  <!-- Load Latex JS -->
  <script src="https://cdn.jsdelivr.net/npm/latex.js@0.11.1/dist/latex.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/latex.js@0.11.1/dist/latex.component.js"></script>
  
</head>

    <script src="/pgm-spring-2025/assets/js/distillpub/template.v2.js"></script>
    <script src="/pgm-spring-2025/assets/js/distillpub/transforms.v2.js"></script>
  </head>

  <d-front-matter>
    <script type="text/json">{
      "title": "Lecture 20-LLMs from a Probabilistic Perspective 1-Implementing a GPT from Scratch",
      "description": "How we evolved from a transformer to a GPT",
      "published": "April 15, 2025",
      "lecturers": [
        
        {
          "lecturer": "Ben Lengerich",
          "lecturerURL": "https://lengerichlab.github.io/"
        }
        
      ],
      "authors": [
        
        {
          "author": "Pranav Vogeti"
        },
        
        {
          "author": "Niko Klekas"
        }
        
      ],
      "editors": [
        
        {
          "editor": "Pranav Vogeti"
        },
        
        {
          "editor": "Niko Klekas"
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  <body>

    <header class="site-header">

  <div class="wrapper">

    <span class="site-title">
      <a class="page-link" href="https://adaptinfer.org/pgm-spring-2025/">STAT 479 PGMs</a>
    </span>

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <a class="page-link" href="/pgm-spring-2025/logistics/">logistics</a>
        <a class="page-link" href="/pgm-spring-2025/lectures/">lectures</a>
        <a class="page-link" href="/pgm-spring-2025/notes/">notes</a>
        <a class="page-link" href="/pgm-spring-2025/calendar/">calendar</a>
        <a class="page-link" href="/pgm-spring-2025/homework/">homework</a>
        <a class="page-link" href="/pgm-spring-2025/project/">project</a>
      </div>
    </nav>

  </div>

</header>



    <div class="page-content">

      <d-title>
        <h1>Lecture 20-LLMs from a Probabilistic Perspective 1-Implementing a GPT from Scratch</h1>
        <p>How we evolved from a transformer to a GPT</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <h1 id="from-transformer-to-gpt">From Transformer to GPT</h1>

<h2 id="the-original-transformer">The Original Transformer</h2>

<p>The original Transformer architecture (Vaswani et al., 2017) was designed for sequence-to-sequence tasks and uses an  framework.</p>

<ul>
  <li>: Maps an input sequence to a contextualized representation.</li>
  <li>: Produces outputs token-by-token using the encoder output and previously generated tokens.</li>
  <li>: Mechanism for learning dependencies within a sequence.</li>
  <li>: Fully relies on self-attention in both encoder and decoder for translation tasks.</li>
</ul>

\[P(Y  X) = _t P(Y_t  Y_{&lt;t}, X)\]

<h2 id="evolving-to-a-gpt">Evolving to a GPT</h2>

<p>GPT simplifies the Transformer by dropping the encoder and using a decoder-only model to model input sequences:</p>

\[P(X) = _t P(X_t  X_{&lt;t})\]

\[_{} _i _t  P_(X_{i,t}  X_{i,&lt;t})\]

<p>This enforces causal structure over the input, forming a directed probabilistic graphical model.</p>

<p><img src="assets/img/notes/lecture-20/directed-pgm.png" alt="image" /></p>

<h1 id="writing-a-gpt">Writing a GPT</h1>

<h2 id="model-configuration">Model Configuration</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="n">torch</span><span class="p">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="mi">1337</span><span class="p">)</span>

<span class="c1"># Training hyperparameters
</span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">max_iters</span> <span class="o">=</span> <span class="mi">5000</span>
<span class="n">eval_interval</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-3</span>
<span class="n">eval_iters</span> <span class="o">=</span> <span class="mi">200</span>

<span class="c1"># Model hyperparameters
</span><span class="kn">from</span> <span class="nn">gpt_config</span> <span class="kn">import</span> <span class="n">GPTConfig</span>
<span class="n">config</span> <span class="o">=</span> <span class="n">GPTConfig</span><span class="p">(</span>
    <span class="n">block_size</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">device</span> <span class="o">=</span> <span class="s">'cuda'</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">'cpu'</span><span class="p">,</span>
    <span class="n">n_embd</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">n_head</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">n_layer</span> <span class="o">=</span> <span class="mi">4</span><span class="p">,</span>
    <span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="p">)</span>
</code></pre></div></div>

<h2 id="load-and-encode-dataset">Load and Encode Dataset</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'input.txt'</span><span class="p">,</span> <span class="s">'r'</span><span class="p">,</span> <span class="n">encoding</span><span class="o">=</span><span class="s">'utf-8'</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="n">text</span> <span class="o">=</span> <span class="n">f</span><span class="p">.</span><span class="n">read</span><span class="p">()</span>
<span class="n">chars</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">text</span><span class="p">)))</span>
<span class="n">config</span><span class="p">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span>

<span class="n">stoi</span> <span class="o">=</span> <span class="p">{</span> <span class="n">ch</span><span class="p">:</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">ch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span> <span class="p">}</span>
<span class="n">itos</span> <span class="o">=</span> <span class="p">{</span> <span class="n">i</span><span class="p">:</span><span class="n">ch</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">ch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">chars</span><span class="p">)</span> <span class="p">}</span>
<span class="n">encode</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">s</span><span class="p">:</span> <span class="p">[</span><span class="n">stoi</span><span class="p">[</span><span class="n">c</span><span class="p">]</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">s</span><span class="p">]</span>
<span class="n">decode</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="s">''</span><span class="p">.</span><span class="n">join</span><span class="p">([</span><span class="n">itos</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">l</span><span class="p">])</span>
</code></pre></div></div>

<h2 id="traintest-split-and-block-sampling">Train/Test Split and Block Sampling</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">encode</span><span class="p">(</span><span class="n">text</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.9</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">))</span>
<span class="n">train_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[:</span><span class="n">n</span><span class="p">]</span>
<span class="n">val_data</span> <span class="o">=</span> <span class="n">data</span><span class="p">[</span><span class="n">n</span><span class="p">:]</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[:</span><span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">):</span>
    <span class="n">context</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:</span><span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">target</span> <span class="o">=</span> <span class="n">y</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>
    <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"For input </span><span class="si">{</span><span class="n">context</span><span class="si">}</span><span class="s">, target is: </span><span class="si">{</span><span class="n">target</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="helper-functions">Helper Functions</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_batch</span><span class="p">(</span><span class="n">split</span><span class="p">):</span>
    <span class="n">data</span> <span class="o">=</span> <span class="n">train_data</span> <span class="k">if</span> <span class="n">split</span> <span class="o">==</span> <span class="s">'train'</span> <span class="k">else</span> <span class="n">val_data</span>
    <span class="n">ix</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">randint</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">data</span><span class="p">)</span> <span class="o">-</span> <span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">,</span> <span class="p">(</span><span class="n">batch_size</span><span class="p">,))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ix</span><span class="p">])</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">stack</span><span class="p">([</span><span class="n">data</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">:</span><span class="n">i</span><span class="o">+</span><span class="n">config</span><span class="p">.</span><span class="n">block_size</span><span class="o">+</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">ix</span><span class="p">])</span>
    <span class="k">return</span> <span class="n">x</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">device</span><span class="p">),</span> <span class="n">y</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>

<span class="o">@</span><span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">()</span>
<span class="k">def</span> <span class="nf">estimate_loss</span><span class="p">():</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="p">[</span><span class="s">'train'</span><span class="p">,</span> <span class="s">'val'</span><span class="p">]:</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">eval_iters</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">eval_iters</span><span class="p">):</span>
            <span class="n">X</span><span class="p">,</span> <span class="n">Y</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="n">split</span><span class="p">)</span>
            <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span>
            <span class="n">losses</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span><span class="p">.</span><span class="n">item</span><span class="p">()</span>
        <span class="n">out</span><span class="p">[</span><span class="n">split</span><span class="p">]</span> <span class="o">=</span> <span class="n">losses</span><span class="p">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="n">model</span><span class="p">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div></div>

<h2 id="training-the-model">Training the Model</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">gpt_zero</span> <span class="kn">import</span> <span class="n">GPT</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GPT</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
<span class="n">m</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">config</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">sum</span><span class="p">(</span><span class="n">p</span><span class="p">.</span><span class="n">numel</span><span class="p">()</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">m</span><span class="p">.</span><span class="n">parameters</span><span class="p">())</span><span class="o">/</span><span class="mf">1e6</span><span class="p">,</span> <span class="s">'M parameters'</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">optim</span><span class="p">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="n">learning_rate</span><span class="p">)</span>

<span class="k">for</span> <span class="nb">iter</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_iters</span><span class="p">):</span>
    <span class="k">if</span> <span class="nb">iter</span> <span class="o">%</span> <span class="n">eval_interval</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="nb">iter</span> <span class="o">==</span> <span class="n">max_iters</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="n">estimate_loss</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="sa">f</span><span class="s">"step </span><span class="si">{</span><span class="nb">iter</span><span class="si">}</span><span class="s">: train loss </span><span class="si">{</span><span class="n">losses</span><span class="p">[</span><span class="s">'train'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">, val loss </span><span class="si">{</span><span class="n">losses</span><span class="p">[</span><span class="s">'val'</span><span class="p">]</span><span class="si">:</span><span class="p">.</span><span class="mi">4</span><span class="n">f</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>

    <span class="n">xb</span><span class="p">,</span> <span class="n">yb</span> <span class="o">=</span> <span class="n">get_batch</span><span class="p">(</span><span class="s">'train'</span><span class="p">)</span>
    <span class="n">logits</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">xb</span><span class="p">,</span> <span class="n">yb</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">(</span><span class="n">set_to_none</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>

<span class="n">context</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">config</span><span class="p">.</span><span class="n">device</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">decode</span><span class="p">(</span><span class="n">m</span><span class="p">.</span><span class="n">generate</span><span class="p">(</span><span class="n">context</span><span class="p">,</span> <span class="n">max_new_tokens</span><span class="o">=</span><span class="mi">2000</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="n">tolist</span><span class="p">()))</span>
</code></pre></div></div>

<h1 id="training-output">Training Output</h1>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>WARWICK:
Yeart, their to you's my 'tcknow your turrothose...
ROMEO:
Wwell-Bethal,
Be lords!   
</code></pre></div></div>

<h1 id="scaling-from-gpt-to-gpt-4">Scaling from “GPT” to GPT-4</h1>

<h2 id="overview">Overview</h2>

<p>This section summarizes the evolution of the GPT family, from a simple, scratch-built model to GPT-4. Each stage reflects major increases in model size, dataset quality, training techniques, and inference strategies. As the architecture scaled, so did the model’s capabilities—enabling GPT to move from character-level toy outputs to world-class language generation and multimodal reasoning.</p>

<h2 id="glossary-of-key-terms">Glossary of Key Terms</h2>

<ul>
  <li>Breaks input text into smaller units. GPT evolved from character-level to Byte-Pair Encoding (BPE), and later to multimodal tokenization.</li>
  <li>Activation functions that determine how neurons in the model fire. GELU is smoother and generally performs better in transformers.</li>
  <li>Size of the vector used to represent each token.</li>
  <li>Parallel attention mechanisms in each transformer block that allow the model to focus on different parts of the input simultaneously.</li>
  <li>Maximum number of tokens the model can consider at once.</li>
  <li>Total number of learnable weights in the model. Higher parameter counts generally allow for more expressive power.</li>
  <li>Text corpus used to train the model. Increased in size and diversity across versions.</li>
  <li>Algorithm used to adjust the model weights during training. Adam is widely used, often with modifications like learning rate warmup and weight decay.</li>
  <li>Method for generating output text. Greedy selects the highest-probability token each time, while top-$k$ introduces diversity.</li>
  <li>A sparsely activated architecture where only subsets of the model are used per input, improving scalability.</li>
  <li>Reinforcement Learning from Human Feedback—a training method that aligns model outputs with human preferences.</li>
</ul>

<h2 id="from-gpt-to-gpt-1">From “GPT” to GPT-1</h2>

<ul>
  <li>

    <ul>
      <li>: Characters $$ Byte-Pair Encoding (BPE)</li>
      <li>: ReLU $$ GELU</li>
      <li>: Tied input/output embeddings</li>
      <li>
        <p>(117M params):</p>
      </li>
      <li>Layers: 4 $$ 12</li>
      <li>Attention heads: 4 $$ 12</li>
      <li>Context length: 32 $$ 512</li>
      <li>Vocabulary: 65 $$ 40,000 tokens</li>
      <li>Embedding dim: 64 $$ 768</li>
    </ul>
  </li>
  <li>

    <ul>
      <li>Dataset: TinyShakespeare (1MB) $$ BookCorpus (5GB)</li>
      <li>Initialization/normalization: Default $$ Tuned</li>
      <li>Optimizer: Adam $$ Adam + warmup + weight decay</li>
    </ul>
  </li>
  <li>

    <ul>
      <li>Sampling: Greedy $$ Top-$k$</li>
    </ul>
  </li>
</ul>

<h2 id="from-gpt-1-to-gpt-2">From GPT-1 to GPT-2</h2>

<ul>
  <li>

    <ul>
      <li>Layers: 12 $$ 48</li>
      <li>Heads: 12 $$ 25</li>
      <li>Embedding dim: 768 $$ 1600</li>
      <li>Context length: 512 $$ 1024</li>
      <li>Vocab size: 40k $$ 50k tokens</li>
    </ul>
  </li>
  <li>BookCorpus (5GB) $$ WebText (40GB)</li>
</ul>

<h2 id="from-gpt-2-to-gpt-3">From GPT-2 to GPT-3</h2>

<ul>
  <li>

    <ul>
      <li>Layers: 48 $$ 96</li>
      <li>Heads: 25 $$ 96</li>
      <li>Embedding dim: 1600 $$ 12{,}288</li>
      <li>Context length: 1024 $$ 2048</li>
    </ul>
  </li>
  <li>WebText (40GB) \(Common Crawl + books, code, etc. (\)570GB)</li>
</ul>

<h2 id="from-gpt-3-to-gpt-4">From GPT-3 to GPT-4</h2>

<ul>
  <li>

    <ul>
      <li>Likely incorporates Mixture-of-Experts (MoE)</li>
      <li>Tokenizer expanded for multimodal inputs (e.g., images)</li>
      <li>
        <p>Scale:</p>

        <ul>
          <li>Parameters: 175B $$ estimated $&gt;$1T</li>
          <li>Context length: 2048 $$ 128{,}000 tokens</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>

    <ul>
      <li>WebText + books, Wikipedia, code, etc. (\(570GB)\) much larger, proprietary dataset (details undisclosed)</li>
      <li>\(13 trillion tokens (\)50TB)</li>
      <li>Reinforcement learning from human feedback (RLHF) + system-level safety mechanisms</li>
    </ul>
  </li>
</ul>

<h2 id="mixture-of-experts-moe">Mixture of Experts (MoE)</h2>

<p>Instead of using a single feedforward layer (FFN) at each transformer block, MoE introduces multiple ``expert’’ FFNs and uses a routing mechanism to dynamically select a subset for each input.</p>

<ul>
  <li>A  analyzes each input token and assigns it to the most relevant expert(s).</li>
  <li>Only a small subset of experts (e.g., 1 or 2 out of 4) is activated per token.</li>
  <li>This allows for , reducing the cost while increasing model capacity.</li>
</ul>

<p>[H]</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>![image](assets/img/notes/lecture-20/moe.png)
</code></pre></div></div>

<ul>
  <li>In a standard transformer (left), each token passes through a single FFN after self-attention.</li>
  <li>In an MoE transformer (right), this FFN is replaced with  (the experts).</li>
  <li>The router selects a few experts to process the token, and the outputs are combined.</li>
  <li>Result: Higher model capacity with the same or lower computational cost at inference time.</li>
</ul>

<p>[H]</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>![image](assets/img/notes/lecture-20/trans_decode.png)
</code></pre></div></div>

<h2 id="mixture-of-experts-probabilistic-view">Mixture of Experts: Probabilistic View</h2>

<p>Model the output as a weighted sum of predictions from multiple expert networks.</p>

<ul>
  <li>
    <p>Let
  \(P(Y  X) = _{m} g_m(X)  P_m(Y  X)\)
  where:</p>

    <ul>
      <li>( P_m(Y  X) ): prediction from expert ( m )</li>
      <li>( g_m(X) ): gating function output (i.e., weight) for expert ( m )</li>
    </ul>
  </li>
  <li>
    <p>Subject to constraints:
  \(_m g_m(X) = 1,    g_m(X)  0   m, X\)
  ensuring a valid probability distribution over experts.</p>
  </li>
  <li>A  computes ( g_m(X) ), deciding how much each expert contributes based on the input.</li>
  <li>
    <p>The  uses these weights to sample or activate experts probabilistically.</p>
  </li>
  <li>
    <p>This framework can be trained via the , where:</p>

    <ul>
      <li>E-step estimates expert responsibilities ( g_m(X) )</li>
      <li>M-step updates expert and gating parameters</li>
    </ul>
  </li>
</ul>

<h2 id="moe-a-unifying-framework-for-ensembles">MoE: A Unifying Framework for Ensembles</h2>

<p>Let the predictive distribution be modeled as:
\(P(Y  X) = _{m} g_m(X)  P_m(Y  X)\)</p>

<ul>
  <li>$g_m(X)$ is a .</li>
  <li>$g_m(X) = {M}$ is a  across experts.</li>
  <li>$g_m(X) = _m$ is a , constant across inputs.</li>
</ul>

<h2 id="moe-error-analysis">MoE: Error Analysis</h2>

<p>Let:
\(P(Y  X) = _{m} g_m(X)  P_m(Y  X)\)</p>

<p>Define the expected prediction (mean function) of the ensemble:
\((x) := [Y  X = x] = _{m} g_m(x) f_m(x)\)</p>

<p>Compare two types of errors:</p>

<ul>
  <li>
\[(x) := (Y - (x))^2\]
  </li>
  <li>
\[(x) := {M} _m (Y - f_m(x))^2\]
  </li>
</ul>

<p>Will minimizing ensemble error $(x)$ also minimize average expert error $(x)$?</p>

<h2 id="moe-diversity-vs-error">MoE: Diversity vs. Error</h2>

<ul>
  <li>Ensemble error: ((x) = (Y - (x))^2)</li>
  <li>
    <p>Average expert error: ((x) = {M} _m (Y - f_m(x))^2)</p>
  </li>
  <li>
    <p>As  increases (x-axis = \% unique training data per expert):</p>

    <ul>
      <li>((x)) (solid line): Ensemble error is minimized at moderate diversity.</li>
      <li>((x)) (dotted line): Average expert error increases with diversity.</li>
      <li>Dashed line: Represents disagreement (variance) between experts’ outputs.</li>
    </ul>
  </li>
</ul>

<p>[H]</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>![image](assets/img/notes/lecture-20/three_lines.png)
</code></pre></div></div>

<ul>
  <li>Expert predictions can individually overfit, but if their errors are uncorrelated, the ensemble prediction can be robust and accurate.</li>
  <li>Diverse (even slightly overfitted) experts cancel out each other’s errors when averaged.</li>
  <li>or purposeful variation among experts improves ensemble generalization.</li>
</ul>

<h2 id="moe-in-large-language-models-llms">MoE in Large Language Models (LLMs)</h2>

<ul>
  <li>Input (  ) is passed through a , which outputs activations ( (x) ).</li>
  <li>The router computes  ( G(x) ), representing how much to weight each expert.</li>
  <li>A small number of experts (e.g., top-1 or top-2) are activated based on ( G(x) ).</li>
  <li>Only the selected FFNNs are evaluated; their outputs ( E(x) ) are weighted by ( G(x) ) and aggregated into the final output ( y ).</li>
</ul>

<p>[H]</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>![image](assets/img/notes/lecture-20/gpt_process.png)
</code></pre></div></div>

<ul>
  <li>Sparse activation means only a few experts need to be loaded and executed per token, reducing compute and memory.</li>
  <li>
    <p>Enables use of massive models without needing to evaluate all parameters at once.</p>
  </li>
  <li>Without proper regularization, some experts dominate while others are underused.</li>
  <li>
    <p>As shown in the plot, more experts (e.g., 128) can reduce training loss but may hurt validation performance due to overfitting or imbalance.</p>
  </li>
  <li>Validation loss increases for larger expert counts, indicating a generalization gap.</li>
  <li>Solution approaches may include load balancing, expert dropout, or routing noise.</li>
</ul>

<h1 id="summary-tables">Summary Tables</h1>

<h2 id="from-transformer-to-gpt-1">From Transformer to GPT</h2>

<table>
  <tbody>
    <tr>
      <td>{</td>
      <td>l</td>
      <td>l</td>
      <td>l</td>
      <td>}</td>
    </tr>
  </tbody>
</table>

<p>&amp;  &amp;  \</p>

<p>Architecture &amp; Encoder-decoder (full) &amp; Decoder-only <br />
Attention &amp; Full self-attention &amp; Masked (causal) self-attention <br />
Positional encoding &amp; Sinusoidal (original) &amp; Learned positional embeddings <br />
Output &amp; Task-specific &amp; Next-token prediction <br />
Training objective &amp; Flexible (e.g., translation) &amp; Language modeling (autoregressive) <br />
Inference &amp; Depends on task &amp; Greedy / sampling for text gen \</p>

<h2 id="from-gpt-1-to-gpt-4">From GPT-1 to GPT-4</h2>

<ul>
  <li>Broad range; largest grew from 1.5B $$ $&gt;$1T parameters</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>- Context length: 512 $$ 128{,}000
- Layers: 12 $$ $&gt;$96
- Attention heads: 12 $$ $&gt;$96
- Embedding dimension: 768 $$ $&gt;$12{,}288
- Vocabulary size: 40k $$ $&gt;$50k tokens
</code></pre></div></div>

<ul>
  <li>Tokenizer: Supports multimodal inputs (e.g., images)</li>
  <li>
    <p>Architecture includes:</p>
  </li>
  <li>Dataset: BookCorpus (5GB) \(Private corpus of 13T tokens (\)50TB)</li>
  <li>Alignment: Reinforcement learning from human feedback (RLHF)</li>
</ul>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Introduction to Probabilistic Graphical Models and Modern Probabilistic AI</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Ben Lengerich</li><li><a class="u-email" href="mailto:TBD@wisc.edu">TBD@wisc.edu</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/AdaptInfer" target="_blank"><i class="fab fa-github"></i> <span class="username">AdaptInfer</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>&copy; Copyright 2025 University of Wisconsin. <br />
        Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

  <d-bibliography src="/pgm-spring-2025/assets/bibliography/2025-04-15-lecture-20.bib">
  </d-bibliography>

  <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/pgm-spring-2025/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.1/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.1/katex.min.js"></script>
<script src="/pgm-spring-2025/assets/js/katex.js"></script>



<!-- Load Anchor JS -->
<script src="//cdnjs.cloudflare.com/ajax/libs/anchor-js/3.2.2/anchor.min.js"></script>
<script>
  anchors.options.visible = 'hover';
  anchors.add('article h2, article h3, article h4, article h5, article h6');
</script>



<!-- Adjust LaTeX JS -->
<script src="/pgm-spring-2025/assets/js/latex.js"></script>


<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/pgm-spring-2025/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/pgm-spring-2025/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', '', 'auto');
ga('send', 'pageview');
</script>


</html>
