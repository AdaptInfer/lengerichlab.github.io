<!DOCTYPE html>
<html>

  <head>
  <meta charset="UTF-8">
  <meta http-equiv="content-language" content="en">
  <meta http-equiv="content-type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <title>STAT 479 PGMs | Lecture 21</title>
  <meta name="description" content="10-708 - Probabilistic Graphical Models - University of Wisconsin-Madison - Spring 2025
">

  <link rel="shortcut icon" href="/pgm-spring-2025/assets/img/favicon.ico">

  <link rel="stylesheet" href="/pgm-spring-2025/assets/css/main.css">
  <link rel="canonical" href="/pgm-spring-2025/notes/lecture-21/">

  
  <!-- Load Latex JS -->
  <script src="https://cdn.jsdelivr.net/npm/latex.js@0.11.1/dist/latex.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/latex.js@0.11.1/dist/latex.component.js"></script>
  
</head>


  <body>

    <header class="site-header">

  <div class="wrapper">

    <span class="site-title">
      <a class="page-link" href="https://adaptinfer.org/pgm-spring-2025/">STAT 479 PGMs</a>
    </span>

    <nav class="site-nav">
      <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
              <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
              <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

      <div class="trigger">
        <a class="page-link" href="/pgm-spring-2025/logistics/">logistics</a>
        <a class="page-link" href="/pgm-spring-2025/lectures/">lectures</a>
        <a class="page-link" href="/pgm-spring-2025/notes/">notes</a>
        <a class="page-link" href="/pgm-spring-2025/calendar/">calendar</a>
        <a class="page-link" href="/pgm-spring-2025/homework/">homework</a>
        <a class="page-link" href="/pgm-spring-2025/project/">project</a>
      </div>
    </nav>

  </div>

</header>



    <div class="page-content">
      <div class="wrapper">
        <div class="post">

  <header class="post-header">
    <h1 class="post-title">Lecture 21</h1>
    <p class="post-meta">April 17, 2025</p>
  </header>

  <article class="post-content">
    <h1 id="lecture-21-llms-from-a-probabilistic-perspective-2-training-on-unlabeled-data">Lecture 21: LLMs from a Probabilistic Perspective 2: Training on Unlabeled Data</h1>

<p>layout: distill
title: Lecture Notes Template
description: An example of a distill-style lecture notes that showcases the main elements.
date: 2025-04-24</p>

<p>lecturers:</p>
<ul>
  <li>name: Ben Lengerich
url: “https://lengerichlab.github.io/”</li>
</ul>

<p>authors:</p>
<ul>
  <li>
    <p>name: Joshua Salinas</p>
  </li>
  <li>
    <p>name: Mitchell Stephens</p>
  </li>
</ul>

<hr />

<h2 id="announcements">Announcements</h2>

<ul>
  <li>Project presentations: April 29 and May 1.</li>
  <li>Submit peer review forms on Canvas each day to earn up to 2% bonus.</li>
  <li>Due by: Friday, May 2.</li>
</ul>

<hr />

<h2 id="unsupervised-training-of-llms">Unsupervised Training of LLMs</h2>

<h3 id="maximum-likelihood-estimation-mle">Maximum Likelihood Estimation (MLE)</h3>

<ul>
  <li>
    <p>GPT models maximize the likelihood of observed sequences:</p>

    <p>$\max_{\theta} \sum_{i=1}^T \log P_{\theta}(x_i \mid x_{&lt;i})$</p>
  </li>
  <li>This is a Directed Probabilistic Graphical Model.</li>
  <li>Predicting even simple tokens (e.g., “is” in a factual sentence) requires grammar, factual knowledge, and context resolution.</li>
  <li>Positional encodings are added to input embeddings to provide sequence order information.</li>
</ul>

<hr />

<h3 id="historical-context">Historical Context</h3>

<ul>
  <li>MLE-based models have existed since 2007.</li>
  <li>2007 Google MLE-based language model had 2 trillion tokens and 300 billion n-grams.</li>
  <li>Early models focused on n-grams, not embeddings.</li>
  <li>Lack of deep representation and contextual modeling.</li>
  <li>The LLM breakthrough came with transformers, large datasets, and GPU acceleration.</li>
</ul>

<hr />

<h3 id="gpu-importance">GPU Importance</h3>

<ul>
  <li>CPUs typically have 4-8 cores with complex control units.</li>
  <li>GPUs split tasks across hundreds of simpler cores for efficient matrix multiplication.</li>
  <li>Downside: GPU cores don’t communicate directly and have slower memory access, but they enable faster computation overall.</li>
</ul>

<p><img src="/pgm-spring-2025/assets/img/notes/lecture-21/notesimage1.png" /></p>

<hr />

<h2 id="scale-and-emergent-capabilities">Scale and Emergent Capabilities</h2>

<ul>
  <li>As model scale increases, new capabilities emerge unexpectedly.</li>
  <li>Examples: in-context learning, chain-of-thought reasoning.</li>
  <li>Refer to: <em>Scaling Laws for Neural Language Models</em> (Kaplan et al. 2021).</li>
</ul>

<blockquote>
  <p><strong>Example:</strong><br />
Predicting “is” in “The capital of France __ Paris” requires:</p>
  <ul>
    <li>Subject-verb agreement</li>
    <li>Recognition of factual structures</li>
    <li>Geographical knowledge</li>
  </ul>
</blockquote>

<hr />

<h3 id="in-context-learning">In-Context Learning</h3>

<ul>
  <li>After training a few models, the LLM learns how to apply (P(x)) efficiently.</li>
  <li><strong>Zero-Shot:</strong> Only given instructions, no examples.</li>
  <li><strong>Few-Shot with Instruction:</strong> Task and examples are provided.</li>
  <li><strong>Few-Shot with Examples Only:</strong> The model must infer the task from examples without instructions.</li>
</ul>

<p><img src="/pgm-spring-2025/assets/img/notes/lecture-21/notesimage2.png" /></p>

<hr />

<h3 id="chain-of-thought">Chain-of-Thought</h3>

<ul>
  <li>Chain-of-Thought prompts the model to explain its reasoning.</li>
  <li>Example: showing all steps to derive a complex equation.</li>
  <li>Helps the model adapt quickly and respond more accurately, especially for complex tasks.</li>
</ul>

<hr />

<h3 id="why-does-this-work">Why Does This Work?</h3>

<ul>
  <li>No definitive proof, but hypotheses include:
    <ul>
      <li>Task identification as implicit Bayesian inference.</li>
      <li>Transformers performing in-context gradient descent.</li>
      <li>Possibly a combination of both.</li>
    </ul>
  </li>
</ul>

<hr />

<h3 id="scale">Scale</h3>

<ul>
  <li>Scaling adds randomness and diversity, which prevents overfitting as models cancel each other’s errors (ensemble effect).</li>
  <li>More parameters increase variance and randomness.</li>
  <li>This can be beneficial for specialized models but makes data filtering and preparation challenging.</li>
  <li>Fitting on poor-quality data leads to bad distributions.</li>
</ul>

<p><img src="/pgm-spring-2025/assets/img/notes/lecture-21/notesimage3.png" /></p>

<hr />

<h2 id="challenges-of-mle">Challenges of MLE</h2>

<h3 id="kl-divergence-minimization">KL Divergence Minimization</h3>

<ul>
  <li>
    <p>MLE objective:</p>

\[\arg\max_{\theta} \mathbb{E}_{x \sim P_{\text{data}}}[\log P_{\theta}(x)] = \arg\min_{\theta} \text{KL}(P_{\text{data}} \| P_{\theta})\]
  </li>
  <li>
    <p><strong>Issues:</strong></p>
    <ul>
      <li>Repetitiveness, memorization of training data.</li>
      <li>Lack of semantic grounding or task objectives.</li>
      <li>Must assign probabilities to incoherent sequences.</li>
      <li>Not task-aware — doesn’t optimize for accuracy or meaning.</li>
    </ul>
  </li>
</ul>

<hr />

<h3 id="entropy-and-confidence">Entropy and Confidence</h3>

<ul>
  <li>
    <p>Token-level entropy:</p>

    <p>$H_t = -\sum_v P(x_t = v \mid x_{&lt;t}) \log P(x_t = v \mid x_{&lt;t})$</p>
  </li>
  <li>Lower entropy = higher confidence.</li>
  <li>Low entropy can lead to generic or repetitive outputs.</li>
  <li>Sampling strategies (greedy, top-k, nucleus) help control diversity.</li>
</ul>

<hr />

<h2 id="links-to-variational-inference">Links to Variational Inference</h2>

<ul>
  <li>
    <p>ELBO (Evidence Lower Bound):</p>

    <p>$\log p(x \mid \theta) \geq \mathbb{E}_{z \sim q}[\log p(x,z \mid \theta)] + H(q)$</p>
  </li>
  <li>Maximizing ELBO encourages higher entropy in latent variables.</li>
  <li>MLE lacks explicit entropy control.</li>
</ul>

<hr />

<h2 id="solutions-to-mle-limitations">Solutions to MLE Limitations</h2>

<ul>
  <li>
    <p><strong>Entropy Regularization:</strong></p>

    <p>$\theta^* = \arg\max_{\theta} \mathbb{E}<em>{x \sim P</em>{\text{data}}}[\log P_{\theta}(x)] + \lambda \cdot H[P_{\theta}(x)]$</p>
  </li>
  <li>
    <p><strong>Smooth Labeling:</strong></p>

    <p>$y’_i = \begin{cases} 1 - \epsilon &amp; y = 1 \\epsilon / (V - 1) &amp; y \neq 1\end{cases}$</p>
  </li>
  <li>
    <p><strong>Other methods:</strong></p>
    <ul>
      <li>Contrastive learning (e.g., noise contrastive estimation).</li>
      <li>Preference- or utility-based training (e.g., RLHF).</li>
      <li>Risk minimization (optimize downstream task loss).</li>
      <li>Scheduled sampling.</li>
      <li>Objectives for coverage/diversity.</li>
      <li>Penalizing low entropy.</li>
    </ul>
  </li>
</ul>

<hr />

<h2 id="references-and-tools">References and Tools</h2>

<ul>
  <li>Radford et al., <em>Improving Language Understanding by Generative Pre-Training</em>.</li>
  <li>Holtzman et al., <em>The Curious Case of Neural Text Degeneration</em>.</li>
  <li>Hugging Face TxT360: <a href="https://huggingface.co/spaces/LLM360/TxT360">https://huggingface.co/spaces/LLM360/TxT360</a></li>
  <li><a href="https://mobidev.biz/blog/gpu-machine-learning-on-premises-vs-cloud">GPU Machine Learning: On-Premises vs. Cloud</a></li>
  <li>Kaplan et al., 2021, <em>Scaling Laws for Neural Language Models</em>.</li>
</ul>

  </article>

  

</div>

      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Introduction to Probabilistic Graphical Models and Modern Probabilistic AI</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name">Ben Lengerich</li><li><a class="u-email" href="mailto:TBD@wisc.edu">TBD@wisc.edu</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"><ul class="social-media-list"><li><a href="https://github.com/AdaptInfer" target="_blank"><i class="fab fa-github"></i> <span class="username">AdaptInfer</span></a></li></ul>
</div>

      <div class="footer-col footer-col-3">
        <p>&copy; Copyright 2025 University of Wisconsin. <br />
        Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme.
</p>
      </div>
    </div>

  </div>

</footer>


    <!-- Load jQuery -->
<script src="//code.jquery.com/jquery-1.12.4.min.js"></script>

<!-- Load Common JS -->
<script src="/pgm-spring-2025/assets/js/common.js"></script>


<!-- Load KaTeX -->
<link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.1/katex.min.css">
<script src="//cdnjs.cloudflare.com/ajax/libs/KaTeX/0.10.1/katex.min.js"></script>
<script src="/pgm-spring-2025/assets/js/katex.js"></script>



<!-- Load Anchor JS -->
<script src="//cdnjs.cloudflare.com/ajax/libs/anchor-js/3.2.2/anchor.min.js"></script>
<script>
  anchors.options.visible = 'hover';
  anchors.add('article h2, article h3, article h4, article h5, article h6');
</script>



<!-- Adjust LaTeX JS -->
<script src="/pgm-spring-2025/assets/js/latex.js"></script>


<!-- Include custom icon fonts -->
<link rel="stylesheet" href="/pgm-spring-2025/assets/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/pgm-spring-2025/assets/css/academicons.min.css">

<!-- Google Analytics -->
<script>
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', '', 'auto');
ga('send', 'pageview');
</script>


  </body>

</html>
